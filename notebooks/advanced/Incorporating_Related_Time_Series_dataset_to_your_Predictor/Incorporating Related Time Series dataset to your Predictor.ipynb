{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Forecast: predicting time-series at scale\n",
    "\n",
    "Forecasting is used in a variety of applications and business use cases: For example, retailers need to forecast the sales of their products to decide how much stock they need by location, Manufacturers need to estimate the number of parts required at their factories to optimize their supply chain, Businesses need to estimate their flexible workforce needs, Utilities need to forecast electricity consumption needs in order to attain an efficient energy network, and enterprises need to estimate their cloud infrastructure needs.\n",
    "<img src=\"https://amazon-forecast-samples.s3-us-west-2.amazonaws.com/common/images/forecast_overview.png\" width=\"98%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "<img src=\"images/forecast_overview.png\" width=\"100%\">\n",
    "\n",
    "In this notebook we will be walking through the all the steps mentioned below.\n",
    "\n",
    "\n",
    "## Table Of Contents\n",
    "* Step 1: [Setup Amazon Forecast](#setup)\n",
    "* Step 2: [Prepare the Datasets](#DataPrep)\n",
    "* Step 2a: [Prepare and Save the Target Time Series](#DataPrepTTS) \n",
    "* Step 2b: [Prepare and save the Related Time Series](#DataPrepRTS) \n",
    "* Step 3: [Create the Dataset Group and Dataset](#DataSet)\n",
    "* Step 4: [Create the Target Time Series Data Import Job](#DataImportTTS)\n",
    "* Step 5: [Create the Related Time Series Data Import Job](#DataImportRTS)\n",
    "* Step 6: [Training a predictor and evaluating its performance](#training)\n",
    "* Step 6a: [Train a Predictor](#train)\n",
    "* Step 6b: [Get Predictor Error Metrics from Backtesting](#predictorErrors)\n",
    "* Step 7: [Create a Forecast](#createForecast)\n",
    "* Step 8: [Query a Forecast](#queryForecast)\n",
    "* Step 9: [Export a Forecast](#exportForecast)\n",
    "* Step 10: [Clean up your Resources](#cleanup)\n",
    "* [Next Steps](#nextSteps)\n",
    "\n",
    "For more informations about APIs, please check the [documentation](https://docs.aws.amazon.com/forecast/latest/dg/what-is-forecast.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup Amazon Forecast<a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "This section sets up the permissions and relevant endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# importing forecast notebook utility from notebooks/common directory\n",
    "sys.path.insert( 0, os.path.abspath(\"../../common\") )\n",
    "import util\n",
    "import util.fcst_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (15.0, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create a new S3 bucket for this lesson</b>\n",
    "- The cell below will create a new S3 bucket with name ending in \"forecast-demo-bike-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# create unique S3 bucket for saving your own data\n",
    "bucket_name = account_id + '-forecast-demo-bike-small'\n",
    "if util.create_bucket(bucket_name, region=region):\n",
    "    print(f\"Success! Created bucket {bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect API sessions\n",
    "session = boto3.Session(region_name=region) \n",
    "s3 = session.client(service_name='s3')\n",
    "forecast = session.client(service_name='forecast') \n",
    "forecastquery = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create IAM Role for Forecast</b> <br>\n",
    "Like many AWS services, Forecast will need to assume an IAM role in order to interact with your S3 resources securely. In the sample notebooks, we use the get_or_create_iam_role() utility function to create an IAM role. Please refer to \"notebooks/common/util/fcst_utils.py\" for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the role to provide to Amazon Forecast.\n",
    "role_name = \"ForecastNotebookRole-Basic\"\n",
    "print(f\"Creating Role {role_name} ...\")\n",
    "role_arn = util.get_or_create_iam_role( role_name = role_name )\n",
    "\n",
    "# echo user inputs without account\n",
    "print(f\"Success! Created role arn = {role_arn.split('/')[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare the Datasets<a class=\"anchor\" id=\"DataPrep\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df = pd.read_csv(\"data/train.csv\", dtype = object)\n",
    "bike_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bike_df.datetime.min())\n",
    "print(bike_df.datetime.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df['count'] = bike_df['count'].astype('float')\n",
    "bike_df['workingday'] = bike_df['workingday'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset happens to span January 01, 2011 to Deceber 31, 2012. We are only going to use about two and a half week's of hourly data to train Amazon Forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_small = bike_df[-2*7*24-24*3:].copy()\n",
    "bike_df_small['item_id'] = \"bike_12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save an item_id for querying later\n",
    "item_id = 'bike_12'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the time series first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_small.plot(x='datetime', y='count', figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the target time series seem to have a drop over weekends. This is a clue for a useful related time series variable.  Let's plot both the target time series and a potential related time series variable `workday` that indicates whether any day is a `workday` or not. \n",
    "\n",
    "More precisely, the new related variable `workday`, $r_t = 1$ if $t$ is a work day and 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "bike_df_small.plot(x='datetime', y='count', ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "bike_df_small.plot(x='datetime', y='workingday', color='red', ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Prepare and Save the Target Time Series<a class=\"anchor\" id=\"DataPrepTTS\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we specify key input data and forecast parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is your forecast horizon in number time units you've selected?\n",
    "# e.g. if you're forecasting in hours, how many months out do you want a forecast?\n",
    "FORECAST_LENGTH = 24\n",
    "\n",
    "# What is your forecast time unit granularity?\n",
    "# Choices are: ^Y|M|W|D|H|30min|15min|10min|5min|1min$ \n",
    "DATASET_FREQUENCY = \"H\"\n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd hh:mm:ss\"\n",
    "# delimiter = ','\n",
    "\n",
    "# What name do you want to give this project?  \n",
    "# We will use this same name for your Forecast Dataset Group name.\n",
    "PROJECT = 'small_bike_demo'\n",
    "DATA_VERSION = '00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = bike_df_small[['item_id', 'datetime', 'count']][:-FORECAST_LENGTH]\n",
    "target_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the output above there are 3 columns of data:\n",
    "\n",
    "1. An Item ID\n",
    "1. The Timestamp\n",
    "1. A Value\n",
    "\n",
    "These are the 3 key required pieces of information to generate a forecast with Amazon Forecast. More can be added but these 3 must always remain present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Prepare and Save the Related Time Series <a class=\"anchor\" id=\"DataPrepRTS\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the related time series, we need to ensure that the related time series covers the whole target time series, as well as the future values as specified by the forecast horizon. More precisely, we need to make sure:\n",
    "```\n",
    "len(related time series) >= len(target time series) + forecast horizon\n",
    "```\n",
    "Basically, all items need to have data start at or before the item start date, and have data until the forecast horizon (i.e. the latest end date across all items + forecast horizon).  Additionally, there should be no missing values in the related time series. The following picture illustrates the desired logic. \n",
    "\n",
    "<img src=\"images/rts_viz.png\">\n",
    "\n",
    "For more details regarding how to prepare your Related Time Series dataset, please refer to the public documentation <a href=\"https://docs.aws.amazon.com/forecast/latest/dg/related-time-series-datasets.html\">here</a>. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_df = bike_df_small[['item_id', 'datetime', 'workingday']]\n",
    "rts_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the length of the related time series is equal to the length of the target time series plus the forecast horizon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(target_df)} + {FORECAST_LENGTH} = {len(rts_df)}\")\n",
    "assert len(target_df) + FORECAST_LENGTH == len(rts_df), \"length doesn't match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check whether there are \"holes\" in the related time series.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(rts_df) == len(pd.date_range(\n",
    "    start=list(rts_df['datetime'])[0],\n",
    "    end=list(rts_df['datetime'])[-1],\n",
    "    freq='H'\n",
    ")), \"missing entries in the related time series\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks fine, the related time series (indicator of whether the current day is a workday or not) is longer than the target time series.  And, the related time series does not have any missing values.\n",
    "\n",
    "The binary working day indicator feature is a good example of a related time series, since it is known at all future time points.  Other examples of related time series include holiday, price, and promotion features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now export them to CSV files and place them into your `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.to_csv(\"data/bike_small.csv\", index= False, header = False)\n",
    "rts_df.to_csv(\"data/bike_small_rts.csv\", index= False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time the data is ready to be sent to S3 where Forecast will use it later. The following cells will upload the data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"bike_small\"\n",
    "\n",
    "s3.upload_file(Filename=\"data/bike_small.csv\", Bucket = bucket_name, Key = f\"{key}/bike.csv\")\n",
    "s3.upload_file(Filename=\"data/bike_small_rts.csv\", Bucket = bucket_name, Key = f\"{key}/bike_rts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create the Dataset Group and Dataset<a class=\"anchor\" id=\"DataSet\"></a>\n",
    "First let's create a dataset group and then update it later to add our datasets.\n",
    "\n",
    "In Amazon Forecast , a dataset is a collection of file(s) which contain data that is relevant for a forecasting task. A dataset must conform to a schema provided by Amazon Forecast. Since data files are imported headerless, it is important to define a schema for your data.\n",
    "\n",
    "More details about `Domain` and dataset type can be found on the [documentation](https://docs.aws.amazon.com/forecast/latest/dg/howitworks-domains-ds-types.html) . For this example, we are using [RETAIL](https://docs.aws.amazon.com/forecast/latest/dg/retail-domain.html) domain with 3 required attributes `timestamp`, `target_value` and `item_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset Group\n",
    "\n",
    "In this task, we define a container name or Dataset Group name, which will be used to keep track of Dataset import files, schema, and all Forecast results which go together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_group = f\"{PROJECT}_{DATA_VERSION}\"\n",
    "print(f\"Dataset Group Name = {dataset_group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arns = []\n",
    "create_dataset_group_response = \\\n",
    "    forecast.create_dataset_group(Domain=\"RETAIL\",\n",
    "                                  DatasetGroupName=dataset_group,\n",
    "                                  DatasetArns=dataset_arns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_group_arn = create_dataset_group_response['DatasetGroupArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Target Schema\n",
    "\n",
    "Next, we specify the schema of our dataset below. Make sure the order of the attributes (columns) matches the raw data in the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files.\n",
    "ts_schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"demand\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Target Dataset \n",
    "\n",
    "Target is a required dataset to use the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_name = f\"{PROJECT}_{DATA_VERSION}_tts\"\n",
    "print(ts_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \\\n",
    "    forecast.create_dataset(Domain=\"RETAIL\",\n",
    "                            DatasetType='TARGET_TIME_SERIES',\n",
    "                            DatasetName=ts_dataset_name,\n",
    "                            DataFrequency=DATASET_FREQUENCY,\n",
    "                            Schema=ts_schema\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_arn = response['DatasetArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset(DatasetArn=ts_dataset_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Related Schema\n",
    "Make sure the order of the attributes (columns) matches the raw data in the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files.\n",
    "rts_schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"workingday\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Related Dataset \n",
    "\n",
    "In this example, we will define a related time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_dataset_name = f\"{PROJECT}_{DATA_VERSION}_rts\"\n",
    "print(rts_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \\\n",
    "    forecast.create_dataset(Domain=\"RETAIL\",\n",
    "                            DatasetType='RELATED_TIME_SERIES',\n",
    "                            DatasetName=rts_dataset_name,\n",
    "                            DataFrequency=DATASET_FREQUENCY,\n",
    "                            Schema=rts_schema\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_dataset_arn = response['DatasetArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset(DatasetArn=rts_dataset_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the dataset group with the datasets we created \n",
    "\n",
    "You can have multiple datasets under the same dataset group. Update it with the datasets we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arns = []\n",
    "dataset_arns.append(ts_dataset_arn)\n",
    "dataset_arns.append(rts_dataset_arn)\n",
    "forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=dataset_arns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Create the Target Time Series Data Import Job<a class=\"anchor\" id=\"DataImportTTS\"></a>\n",
    "\n",
    "Now that Forecast knows how to understand the CSV we are providing, the next step is to import the data from S3 into Amazon Forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_path = f\"s3://{bucket_name}/{key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_s3_data_path = f\"{s3_data_path}/bike.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_import_job_response = \\\n",
    "    forecast.create_dataset_import_job(DatasetImportJobName=dataset_group,\n",
    "                                       DatasetArn=ts_dataset_arn,\n",
    "                                       DataSource= {\n",
    "                                         \"S3Config\" : {\n",
    "                                             \"Path\": ts_s3_data_path,\n",
    "                                             \"RoleArn\": role_arn\n",
    "                                         } \n",
    "                                       },\n",
    "                                       TimestampFormat=TIMESTAMP_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_import_job_arn=ts_dataset_import_job_response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of dataset, when the status change from **CREATE_IN_PROGRESS** to **ACTIVE**, we can continue to next steps. Depending on the data size. It can take 10 mins to be **ACTIVE**. This process will take 5 to 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Create a Related Time Series Data Import Job<a class=\"anchor\" id=\"DataImportRTS\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_s3_data_path = f\"{s3_data_path}/bike_rts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_dataset_import_job_response = \\\n",
    "    forecast.create_dataset_import_job(DatasetImportJobName=dataset_group,\n",
    "                                       DatasetArn=rts_dataset_arn,\n",
    "                                       DataSource= {\n",
    "                                         \"S3Config\" : {\n",
    "                                             \"Path\": rts_s3_data_path,\n",
    "                                             \"RoleArn\": role_arn\n",
    "                                         } \n",
    "                                       },\n",
    "                                       TimestampFormat=TIMESTAMP_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_dataset_import_job_arn=rts_dataset_import_job_response['DatasetImportJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of dataset, when the status change from **CREATE_IN_PROGRESS** to **ACTIVE**, we can continue to next steps. Depending on the data size. It can take 10 mins to be **ACTIVE**. This process will take 5 to 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_dataset_import_job(DatasetImportJobArn=rts_dataset_import_job_arn))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Training a predictor and evaluating its performance<a class=\"anchor\" id=\"train\"></a>\n",
    "\n",
    "Once the datasets are specified with the corresponding schema, Amazon Forecast will automatically aggregate all the relevant pieces of information for each item, such as sales, price, promotions, as well as categorical attributes, and generate the desired dataset. Amazon Forecast creates predictors, which involves applying the optimal combination of algorithms to each time series in your datasets.\n",
    "ML experts train separate models for different parts of their dataset to improve forecasting accuracy. This process of segmenting your data and applying different algorithms can be very challenging for non-ML experts. Forecast uses ML to learn not only the best algorithm for each item, but the best ensemble of algorithms for each item.\n",
    "\n",
    "## How to evaluate a forecasting model?\n",
    "\n",
    "Before moving forward, let's first introduce the notion of *backtest* when evaluating forecasting models. The key difference between evaluating forecasting algorithms and standard ML applications is that we need to make sure there is no future information gets used in the past. In other words, the procedure needs to be causal. \n",
    "\n",
    "<img src=\"https://amazon-forecast-samples.s3-us-west-2.amazonaws.com/common/images/backtest.png\" width=70%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6a.  Train a Predictor <a class=\"anchor\" id=\"trainaAutoPred\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_name = f\"{PROJECT}_{DATA_VERSION}_predictor\"\n",
    "print(f\"Predictor Name = {predictor_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecast.create_auto_predictor(PredictorName = predictor_name,\n",
    "                                   ForecastHorizon = FORECAST_LENGTH,\n",
    "                                   ForecastFrequency = DATASET_FREQUENCY,\n",
    "                                   DataConfig = {\n",
    "                                       'DatasetGroupArn': dataset_group_arn, \n",
    "                                    },\n",
    "                                   ExplainPredictor = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_arn = response['PredictorArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the predictor. When the status change from **CREATE_IN_PROGRESS** to **ACTIVE**, we can continue to next steps. Depending on data size, model selection and choice of hyper parameters tuning，it can take several hours to be **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_auto_predictor(PredictorArn=predictor_arn))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_auto_predictor(PredictorArn=predictor_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6b. Get Predictor Error Metrics from Backtesting <a class=\"anchor\" id=\"predictorErrors\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the predictors, we can query the errors given by the backtest scenario and have a quantitative understanding of the performance of the algorithm. In the cells below, we get the predictor error metrics. \n",
    "\n",
    "We're not demoing it in this notebook, but there is also an Export Predictor Backtest files job you can trigger.  This will save Predictor Error Metrics and also save Item-level Backtest Forecasts to an S3 bucket of your choice.  This is useful in case you want to use custom metric calculations on particular groups of items.\n",
    "<a href=\"https://github.com/aws-samples/amazon-forecast-samples/tree/master/notebooks/advanced/Item_Level_Accuracy\" target=\"_blank\">See advanced/Item_Level_Accuracy notebook</a>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics = forecast.get_accuracy_metrics(PredictorArn=predictor_arn)\n",
    "error_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Create a Forecast <a class=\"anchor\" id=\"createForecast\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_name = f\"{PROJECT}_{DATA_VERSION}_forecast\"\n",
    "print(f\"Forecast Name = {predictor_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecast.create_forecast(ForecastName=forecast_name,PredictorArn=predictor_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_arn = response['ForecastArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the forecast process, when the status change from **CREATE_IN_PROGRESS** to **ACTIVE**, we can continue to next steps. Depending on data size, model selection and choice of hyper parameters tuning，it can take several hours to be **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = util.wait(lambda: forecast.describe_forecast(ForecastArn=forecast_arn))\n",
    "assert status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_forecast(ForecastArn=forecast_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8. Query a Forecast<a class=\"anchor\" id=\"queryForecast\"></a>\n",
    "\n",
    "Once created, the forecast results are ready and you view them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecastquery.query_forecast(\n",
    "    ForecastArn=forecast_arn,\n",
    "    Filters={\"item_id\": item_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'data/bike_small.csv'\n",
    "exact = util.load_exact_sol(fname, item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_forecasts(response, exact)\n",
    "plt.title(\"Auto Predictor Forecast\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9. Export a Forecast<a class=\"anchor\" id=\"exportForecast\"></a>\n",
    "\n",
    "Forecasts can be exported to your own S3 bucket of choice.  You may need to use these in downstream Supply Chain processes.  Or, perhaps you just want to import them into a BI tool to visualize and socialize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_export_name = f\"{PROJECT}_{DATA_VERSION}_forecast_export\"\n",
    "forecast_export_path = f\"{s3_data_path}/{forecast_export_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecast.create_forecast_export_job(ForecastExportJobName=forecast_export_name,\n",
    "                                        ForecastArn=forecast_arn,\n",
    "                                        Destination={\n",
    "                                            \"S3Config\" : {\n",
    "                                                \"Path\": forecast_export_path,\n",
    "                                                \"RoleArn\": role_arn\n",
    "                                            }\n",
    "                                        })\n",
    "forecast_export_arn = response['ForecastExportJobArn']\n",
    "forecast_export_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10. Clean up your Resources<a class=\"anchor\" id=\"cleanup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have completed the above steps, we can start to cleanup the resources we created. All delete jobs, except for `delete_dataset_group` are asynchronous, so we have added the helpful `wait_till_delete` function. \n",
    "Resource Limits documented <a href=\"https://docs.aws.amazon.com/forecast/latest/dg/limits.html\">here</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This needs to be un-commented for clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete forecast export jobs\n",
    "# util.wait_till_delete(lambda: forecast.delete_forecast_export_job(ForecastExportJobArn = forecast_export_arn))\n",
    "\n",
    "# # Delete forecasts\n",
    "# util.wait_till_delete(lambda: forecast.delete_forecast(ForecastArn = forecast_arn))\n",
    "\n",
    "# # Delete predictors\n",
    "# util.wait_till_delete(lambda: forecast.delete_predictor(PredictorArn = predictor_arn))\n",
    "\n",
    "# # Delete the target time series and related time series dataset import jobs\n",
    "# util.wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=ts_dataset_import_job_arn))\n",
    "# util.wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=rts_dataset_import_job_arn))\n",
    "\n",
    "# # Delete the target time series and related time series datasets\n",
    "# util.wait_till_delete(lambda: forecast.delete_dataset(DatasetArn=ts_dataset_arn))\n",
    "# util.wait_till_delete(lambda: forecast.delete_dataset(DatasetArn=rts_dataset_arn))\n",
    "\n",
    "# # Delete dataset group\n",
    "# util.wait_till_delete(lambda: forecast.delete_dataset_group(DatasetGroupArn=dataset_group_arn))\n",
    "\n",
    "# # Delete your file in S3\n",
    "# boto3.Session().resource('s3').Bucket(bucket_name).Object(key).delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps<a class=\"anchor\" id=\"nextSteps\"></a>\n",
    "\n",
    "Congratulations!! You've trained your first Amazon Forecast model and generated your first forecast!!\n",
    "\n",
    "To dive deeper, here are a couple options for further evaluation:\n",
    "<ul>\n",
    "    <li>Example how to use a notebook and Predictor Backtest Forecasts to evaluate all items at once using custom metrics: <a href=\"https://github.com/aws-samples/amazon-forecast-samples/tree/master/notebooks/advanced/Item_Level_Accuracy\" target=\"_blank\">Item_Level_Accuracy notebook</a></li>\n",
    "    <li>Example how to use our built-in, hosted-by-AWS weather data: <a href=\"https://github.com/aws-samples/amazon-forecast-samples/blob/master/notebooks/advanced/Weather_index\" target=\"_blank\">Training your model with Weather Index </a></li>\n",
    "    <li>Finally, for a production-level example, how to use Amazon QuickSight to visualize either Predictor Backtest Forecasts and/or Forecasts so you can share and socialize the results with others <a href=\"https://aws.amazon.com/solutions/implementations/improving-forecast-accuracy-with-machine-learning/?did=sl_card&trk=sl_card\" target=\"_blank\">see our automation solution Improving Forecast Accuracy</a></li>\n",
    "    <li><a href=\"https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=forecast-stack&t[…]acy-with-machine-learning-demo.template\" target=\"_blank\">Quick launch link for above automation</a></li>\n",
    "    </ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
