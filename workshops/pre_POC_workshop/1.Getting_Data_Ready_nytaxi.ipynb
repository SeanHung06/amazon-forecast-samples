{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data input for Amazon Forecast\n",
    "\n",
    "> *This notebook should work with either the `Python 3 (Data Science)` kernel in SageMaker Studio, or `conda_python3` in classic SageMaker Notebook Instances*\n",
    "\n",
    "Forecasting is used in a variety of applications and business use cases: For example, retailers need to forecast the sales of their products to decide how much stock they need by location, Manufacturers need to estimate the number of parts required at their factories to optimize their supply chain, Businesses need to estimate their flexible workforce needs, Utilities need to forecast electricity consumption needs in order to attain an efficient energy network, and enterprises need to estimate their cloud infrastructure needs.\n",
    "<img src=\"https://amazon-forecast-samples.s3-us-west-2.amazonaws.com/common/images/forecast_overview.png\" width=\"98%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook provides a template of typical steps required to prepare data input for Amazon Forecast. So, this is part of \"Upload your data to Amazon Forecast\" in the diagram above.  Your data may need additional Prepare steps and/or you may not need all the steps in this notebook.<br>\n",
    "\n",
    "You will need lots of customization in the RTS section.  Otherwise, look for following comment string to indicate places you will need to customize this notebook for your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1 Read data</b>. Modify this section to read your data.\n",
    "\n",
    "<b>Step 2 Make forecast choices</b>. You need to make some decisions.  How many time units in future do you want to make forecasts?  E.g. if time unit is Hour, then if you want to forecast out 1 week that would be 24*7 = 168 hours.  You'll also be asked if you want integer or float target_values.  And if 0's really mean 0 or are they missing data?\n",
    "\n",
    "<b>Step 10 Aggregate based on the time granularity of your data.</b> Possible aggregations are minute, hour, day, week, month.  See <a href=\"https://docs.aws.amazon.com/forecast/latest/dg/howitworks-datasets-groups.html#howitworks-data-alignment\" target=\"_blank\">documenatation. </a>\n",
    "\n",
    "<b>Step 12 Visualization time series and validate the time granularity you chose makes sense.</b>  Too low-granularity means the time series looks like white noise and the data could benefit by aggregation at higher-time-level.  For example, you might start to notice time series cyclical patterns when aggregating historical sales by-hour instead of of by-minute.  Alternate between Step 7 Visualization and Step 5 Aggregation until you are happy with the chosen time-grandularity for your forecasts.\n",
    "\n",
    "<b>Steps 10 (Aggregate) and 13 (Split train) include a set of common checks on your data for forecasting.</b> One common mistake is customers think they should \"try Amazon Forecast with a small sample of data\".  If the majority of your time series have fewer than 300 data points, Amazon Forecast will fail, because Amazon Forecast is designed for deep-learning AI forecasting algorithms, which require many data points (typically 1000+) per time series.  Small data is better off using traditional, open-source forecast methods such as ETS (available in Excel), ARIMA, or Prophet.  Small data is not a good fit for Amazon Forecast.  If you really want to try Amazon Forecast, but your data fails the Error Checks consider: \n",
    "\n",
    "<ul>\n",
    "    <li>Can you get more data such that each time series will have longer history with more data points?  If so, return to Step 1.</li>\n",
    "    <li>Can you combine items into fewer item_ids such that each time series will have longer history with more data points?  If so, return to Step 1.</li>\n",
    "    <li>Can you reduce forecast dimensions, such as use item_id only and drop location_id?  If so, return to Step 5.</li>\n",
    "    <li>Can you drop to a lower time-frequency without your data turning into white noise?  Check your data again using Step 10 Aggregate and Step 12 Visualize. </li>\n",
    "</ul>\n",
    " \n",
    "It may be that you find your data is not a good fit for Amazon Forecast.  In that case, it's better that you discover this early.\n",
    "\n",
    "<b>In steps 14-20, we will save headerless Target Time Series (TTS), Item metadata (IM), Related Time Series (RTS) to S3</b> so we can trigger Amazon Forecast automation, see\n",
    "    <a href=\"https://github.com/aws-samples/amazon-forecast-samples/blob/master/workshops/pre_POC_workshop/install-forecast-solution.md\" target=\"_blank\">Workshop Instructions Install Automation and Demo</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  bucket steps so list doesn't look so long! <br>\n",
    "TODO:  Add data normalization step\n",
    "\n",
    "# Table of Contents \n",
    "* Step 0: [Set up and install libraries](#setup)\n",
    "* Step 1: [Read data](#read)\n",
    "* Step 2: [Correct dtypes](#fix_dtypes)\n",
    "* Step 3: [Make forecast choices](#choices)\n",
    "* Step 4: [Drop null item_ids](#drop_null_items)\n",
    "* Step 5: [Drop null timestamps](#drop_null_times) \n",
    "* Step 7: [Inspect and treat extremes](#treat_extremes) \n",
    "* Step 8: [Optional - Round negative targets up to 0](#round_negatives) \n",
    "* Step 9: [Optional - Convert negative targets to_nan](#negatives_to_nan) \n",
    "* Step 10: [Aggregate at chosen frequency](#groupby_frequency) \n",
    "* Step 11: [Typical Retail scenario: Find top-moving items](#top_moving_items)\n",
    "* Step 12: [Visualize time series](#visualize) \n",
    "* Step 13: [Split train/test data](#split_train_test)\n",
    "* Step 14: [Prepare and save Target Time Series](#TTS) \n",
    "* Step 15: [Remove time series with no target values at all](#TTS_remove_all0)\n",
    "* Step 16: [Remove time series with end of life](#TTS_remove_end_of_life)\n",
    "* Step 17: [Remove time series with fewer than 5 data points](#TTS_remove_too_few_data_points)\n",
    "* Step 18: [Optional - Assemble and save TTS_sparse, TTS_dense](#TTS-dense_sparse)\n",
    "* Step 19: [Optional - Assemble and save TTS_top, TTS_slow](#TTS_top)\n",
    "* Step 20: [Assemble and save RTS (if any)](#RTS)\n",
    "* Step 21: [Classify time series](#Classify)\n",
    "* Step 22: [Optional - Assemble and save TTS_smooth, TTS_erratic, TTS_intermittent, TTS_lumpy](#TTS_classes)\n",
    "* Step 23: [Assemble and save metadata (if any)](#IM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used in these notebooks: NYC Taxi trips open data\n",
    "\n",
    "Given hourly historical taxi trips data for NYC, your task is to predict #pickups in next 7 days, per hour and per pickup zone.  <br>\n",
    "\n",
    "<ul>\n",
    "<li>Original data source:  <a href=\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\" target=\"_blank\"> https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page</a> </li>\n",
    "<li>AWS-hosted public source:  <a href=\"https://registry.opendata.aws/nyc-tlc-trip-records-pds/\" target=\"_blank\">https://registry.opendata.aws/nyc-tlc-trip-records-pds/ </a> </li>\n",
    "<li>AWS managed weather data ingestion as a service that is bundled with Amazon Forecast, aggregated by location and by hour.  Initially only for USA and Europe, but depending on demand, possibly in the future for other global regions. </li>\n",
    "<li>Data used:  Yellow taxis dates: 2018-12 through 2020-02 to avoid COVID effects </li>\n",
    "</ul>\n",
    "\n",
    " \n",
    "### Features and cleaning\n",
    "Note: ~5GB Raw Data has already been cleaned and joined using AWS Glue (tutorials to be created in future). \n",
    "<ul>\n",
    "    <li>Join shape files Latitude, Longitude</li>\n",
    "    <li>Add Trip duration in minutes</li>\n",
    "    <li>Drop negative trip distances, 0 fares, 0 passengers, less than 1min trip durations </li>\n",
    "    <li>Drop 2 unknown zones ['264', '265']\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0:  Set up and install libraries <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "This notebook will use a range of Python's built-in modules, some open source libraries from AWS and third parties, and some local 'utility' modules where we've packaged some common functions to help keep the notebook concise.\n",
    "\n",
    "If you'd like to dive deeper, you'll be able to find the source code for all referenced `util` and `local_util` functions in this code repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally install dask for faster joins when df is large\n",
    "!pip install \"dask[dataframe]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python standard built-ins:\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Open-source libraries:\n",
    "import boto3  # The AWS SDK for Python\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib as mpl  # Graph plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np  # Numerical processing\n",
    "import pandas as pd  # Dataframe (tabular data) processing\n",
    "import seaborn as sns  # Graph plotting\n",
    "\n",
    "print('matplotlib: {}'.format(mpl.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "\n",
    "# Local code:\n",
    "import local_util  # From the local_util/ folder\n",
    "sys.path.insert( 0, os.path.abspath(\"../../notebooks/common\") )\n",
    "import util  # From this repository's /notebooks/common folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll perform some initial display configuration and choose a color palette, for later visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # Seed random number generator for consistent results\n",
    "\n",
    "local_util.plotting.configure_pandas_display()\n",
    "\n",
    "color_pal = sns.color_palette(\"colorblind\", 6).as_hex()\n",
    "colorblind6 = ','.join(color_pal).split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create a new S3 bucket for this lesson</b>\n",
    "- The cell below will create a new S3 bucket with name ending in \"forecast-demo-taxi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique S3 bucket for saving your own data\n",
    "\n",
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "region = boto3.Session(\"s3\").region_name\n",
    "print(f\"region = {region}\")\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# create unique S3 bucket for saving your own data\n",
    "bucket_name = account_id + '-forecast-demo-taxi'\n",
    "print(f\"bucket_name = {bucket_name}\")\n",
    "util.create_bucket(bucket_name, region)\n",
    "    \n",
    "# create prefix for organizing your new bucket\n",
    "prefix = \"nyc-taxi-trips\"\n",
    "print(f\"using folder '{prefix}'\")\n",
    "\n",
    "# ...and a folder for local files\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect API sessions\n",
    "session = boto3.Session(region_name=region) \n",
    "s3 = session.client(service_name='s3')\n",
    "forecast = session.client(service_name='forecast') \n",
    "forecastquery = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check you can communicate with Forecast APIs\n",
    "forecast.list_predictors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create IAM Role for Forecast</b> <br>\n",
    "Like many AWS services, Forecast will need to assume an IAM role in order to interact with your S3 resources securely. In the sample notebooks, we use the get_or_create_iam_role() utility function to create an IAM role. Please refer to \"notebooks/common/util/fcst_utils.py\" for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sagemaker session first, if not found create a role\n",
    "try:\n",
    "    from sagemaker import get_execution_role\n",
    "    role_arn = get_execution_role()\n",
    "except:\n",
    "    # Create the role to provide to Amazon Forecast.\n",
    "    role_name = \"ForecastNotebookRole-Basic\"\n",
    "    print(f\"Creating Role {role_name} ...\")\n",
    "    role_arn = util.get_or_create_iam_role( role_name = role_name )\n",
    "    \n",
    "# echo user inputs without account\n",
    "print(f\"Success! Using role arn = {role_arn.partition('/')[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Read data <a class=\"anchor\" id=\"read\"></a>\n",
    "\n",
    "The first thing we're going to do is read the headerless .csv file.  Then we need to identify which columns map to required Amazon Forecast inputs.\n",
    "\n",
    "<img src=\"https://amazon-forecast-samples.s3-us-west-2.amazonaws.com/common/images/nyctaxi_map_fields.png\" width=\"82%\">\n",
    "<br>\n",
    "\n",
    "<b>In order to use Weather Index, you need a geolocation-type column.</b>  The geolocation-type column connects your locations to geolocations, and can be 5-digit postal code or latitude_longitude.  For more details, see:\n",
    "<ul>\n",
    "    <li><a href=\"https://docs.aws.amazon.com/forecast/latest/dg/weather.html\" target=\"_blank\">Link to documentation about geolocations</a></li>\n",
    "    <li><a href=\"https://aws.amazon.com/blogs/machine-learning/amazon-forecast-weather-index-automatically-include-local-weather-to-increase-your-forecasting-model-accuracy/\" target=\"_blank\">Our Weather blog, which shows UI steps.</a></li>\n",
    "</ul>\n",
    "\n",
    "The cell below shows an example reading headerless csv file with lat_lon geolocation column, \"pickup_geolocation\".  The rest of this notebook writes headerless csv files to be able to use automation.  If you are not planning on using the automation solution, .csv files with headers are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "\n",
    "## Read cleaned, joined, featurized data from Glue ETL processing\n",
    "df_raw = pd.read_csv(\n",
    "    \"s3://amazon-forecast-samples/data_prep_templates/clean_features.csv\",\n",
    "    parse_dates=True,\n",
    "    header=None,\n",
    "    dtype={\n",
    "        0: \"str\",\n",
    "        1: \"str\",\n",
    "        2: \"str\",\n",
    "        3: \"str\",\n",
    "        4: \"int32\",\n",
    "        5: \"float64\",\n",
    "        6: \"str\",\n",
    "        7: \"str\",\n",
    "        8: \"str\",\n",
    "    },\n",
    "    names=[\n",
    "        \"pulocationid\", \"pickup_hourly\", \"pickup_day_of_week\", \"day_hour\", \"trip_quantity\",\n",
    "        \"mean_item_loc_weekday\", \"pickup_geolocation\", \"pickup_borough\", \"binned_max_item\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# drop duplicates\n",
    "print(df_raw.shape)\n",
    "df_raw.drop_duplicates(inplace=True)\n",
    "\n",
    "df_raw['pickup_hourly'] = pd.to_datetime(df_raw[\"pickup_hourly\"], format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "print(df_raw.shape)\n",
    "print(df_raw.dtypes)\n",
    "start_time = df_raw.pickup_hourly.min()\n",
    "end_time = df_raw.pickup_hourly.max()\n",
    "print(f\"Min timestamp = {start_time}\")\n",
    "print(f\"Max timestamp = {end_time}\")\n",
    "df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "\n",
    "# map expected column names\n",
    "item_id = \"pulocationid\"\n",
    "target_value = \"trip_quantity\"\n",
    "timestamp = \"pickup_hourly\"\n",
    "location_id = None\n",
    "geolocation = \"pickup_geolocation\"\n",
    "\n",
    "if location_id is None:\n",
    "    use_location = False\n",
    "else:\n",
    "    use_location = True\n",
    "print(f\"use_location = {use_location}\")\n",
    "\n",
    "# specify array of dimensions you'll use for forecasting\n",
    "if use_location:\n",
    "    forecast_dims = [timestamp, location_id, item_id]\n",
    "else:\n",
    "    forecast_dims = [timestamp, item_id]\n",
    "print(f\"forecast_dims = {forecast_dims}\")\n",
    "dims_except_timestamp = [i for i in forecast_dims if i != timestamp]\n",
    "print(f\"dims_except_timestamp = {dims_except_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape = df_raw.shape \n",
    "original_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Correct dtypes <a class=\"anchor\" id=\"fix_dtypes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "# # correct dtypes\n",
    "# df_raw['ReportingMonth'] = pd.to_datetime(df_raw[\"ReportingMonth\"], format=\"%Y-%m-%d\", errors='coerce')\n",
    "# # Use the new pandas Integer type\n",
    "# df_raw.ProductGroup3Id = df_raw.ProductGroup3Id.astype('Int64').astype(str)\n",
    "# # df_raw.ProductGroup3Id = df_raw.ProductGroup3Id.astype('Int64')\n",
    "print(df_raw.shape)\n",
    "print(df_raw.dtypes)\n",
    "\n",
    "# df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Make forecast choices <a class=\"anchor\" id=\"choices\"></a>\n",
    "\n",
    "Below, you need to make some choices.  First, you will be asked how you want to treat target values (the values you're forecasting). \n",
    "<ol>\n",
    "    <li><b>Do you want your target values to be floating point numbers or integers? Pro-tip: If your raw data values are already integers or if you need to forecast new items/cold-starts, leave this setting as 'False' to do nothing; otherwise, best practice is set equal 'True'.</b>  If you have floating point numbers, you won't be able to use negative-binomial distribution later in the DeepAR+ algorithm. Setting 'False' will leave raw data values as they are. </li>\n",
    "    <li><b>Decide if you want to convert 0's to nulls. Pro-tip:  Use setting 'True',</b> then let Amazon Forecast  <a href=\"https://docs.aws.amazon.com/forecast/latest/dg/howitworks-missing-values.html\" target=\"_blank\">do automatic null-filling through its Featurization settings.</a>. Setting 'True' will convert 0's to nulls. It means you think some of your 0's were missing data instead of actual 0's. Warning: if you have cold-starts, or new product introductions, they may get dropped. You will need to manually re-add them to only the Inference data, since they should be dropped from Train data.  However, if you believe in your data '0' always means '0', then type 'False' to do nothing to 0's. </li>\n",
    "    <li><b>Decide if you want to replace extreme values with mean?</b>  If you think you only have one or two extreme values it might make sense to replace them. On the other hand, if you have quite a few, that is indication those values are not really extreme.</li>\n",
    "    <li><b>Do you want to generate future RTS that extends into future?</b>  If you set this to True, all data will be used for Training.  If you set it to False, a hold-out of length Forecast Horizon will be used to alculte RTS.</li>\n",
    "    <li><b>What is the time granularity for your forecasts?</b>. For example, if your time unit is Hour, answer = \"H\". </li>\n",
    "    <li><b>How many time units do you want to forecast?</b>. For example, if your time unit is Hour, then if you want to forecast out 1 week, that would be 24*7 = 168 hours, so answer = 168. </li>\n",
    "    <li><b>What is the first date you want to forecast?</b>  Training data will be cut short 1 time unit before the desired first forecast snapshot date.  For example, if the granularity of your data is \"D\" and you want your first forecast to happen on Feb 8, 2019, then the last timestamp for training will be Feb 7, 2019.</li>\n",
    "    <li><b>Think of a name you want to give this experiment</b>, so all files will have the same names.  You should also use this same name for your Forecast DatasetGroup name, to set yourself up for reproducibility. </li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORECAST SETTINGS\n",
    "###################\n",
    "# INSTRUCTIONS: \n",
    "# 1. replace occurences \"xxx=+FORECAST_LENGTH\" with correct dictionary value \n",
    "# 2. run this cell 1x, look at suggested \"snapshot_date\"\n",
    "# 3. use suggested \"snapshot_date\", run again.\n",
    "###################\n",
    "\n",
    "# Round target values to integers\n",
    "target_to_integer = False\n",
    "\n",
    "# Replace 0's with nulls\n",
    "replace_all_zeroes_with_null = False\n",
    "\n",
    "# Replace extremes with mean of last 3 months\n",
    "replace_extremes_with_mean = False\n",
    "\n",
    "# Create RTS with unknown future data\n",
    "# Note: if you set this to True, all known data will be used for Training\n",
    "# Note: if you set this to False, a hold-out of length Forecast Horizon will be used to calculate RTS\n",
    "create_future_RTS_with_unknown_data = False\n",
    "\n",
    "# What is your forecast time unit granularity?\n",
    "# Choices are: ^Y|M|W|D|H|30min|15min|10min|5min|1min$ \n",
    "FORECAST_FREQ = \"H\"\n",
    "\n",
    "# what is your forecast horizon in number time units you've selected?\n",
    "# e.g. if you're forecasting in hours, how many hours out do you want a forecast?\n",
    "FORECAST_LENGTH = 168\n",
    "\n",
    "# What is the first date you want to forecast? \n",
    "# Training data will be cut short 1 time unit before the desired first forecast snapshot date\n",
    "# get snapshot date (date of 1st forecast) as last time minus forecast horizon\n",
    "AF_freq_to_dateutil_freq = {\"Y\":\"years\", \"M\":\"hours\", \"W\":\"weeks\", \"D\":\"days\", \"H\":\"hours\"}\n",
    "\n",
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "###################\n",
    "# INSTRUCTIONS: replace \"xxx=FORECAST_LENGTH\" with correct dictionary value from above\n",
    "#               example if FORECAST_FREQ=\"W\" then use \"weeks=FORECAST_LENGTH\"\n",
    "###################\n",
    "end_time_train = df_raw[timestamp].max() - relativedelta(hours=FORECAST_LENGTH)\n",
    "snapshot_date = end_time_train.date() + relativedelta(days=1)\n",
    "print(f\"Suggested snapshot date = {snapshot_date}\")\n",
    "\n",
    "# Run entire cell 1x, to view the \"Suggested snapshot date\"\n",
    "# Change snapshot date below to match suggested\n",
    "SNAPSHOT_DATE = datetime.datetime(2020, 2, 23, 0, 0, 0)  \n",
    "\n",
    "# What name do you want to give this experiment?  \n",
    "# Be sure to use same name for your Forecast Dataset Group name.\n",
    "EXPERIMENT_NAME = \"nyctaxi\"\n",
    "DATA_VERSION = 1\n",
    "\n",
    "# print some validation back to user\n",
    "print(f\"Convert your frequency to python dateutil = {AF_freq_to_dateutil_freq[FORECAST_FREQ]}\")\n",
    "print(f\"Forecast horizon = {FORECAST_LENGTH} {AF_freq_to_dateutil_freq[FORECAST_FREQ]}\")\n",
    "\n",
    "AF_freq_to_dateutil_freq = {\"Y\":\"years\", \"M\":\"hours\", \"W\":\"weeks\", \"D\":\"days\", \"H\":\"hours\"}\n",
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "###################\n",
    "# INSTRUCTIONS: replace \"xxx=+FORECAST_LENGTH\" with correct dictionary value from above\n",
    "#               example if FORECAST_FREQ=\"W\" then use \"weeks=+FORECAST_LENGTH\"\n",
    "###################\n",
    "snapshot_end = SNAPSHOT_DATE + relativedelta(hours=+FORECAST_LENGTH)\n",
    "snapshot_end = snapshot_end - relativedelta(hours=1)\n",
    "print(f\"Training data end date = {end_time_train}\")\n",
    "print(f\"Forecast start date = {SNAPSHOT_DATE}\")\n",
    "print(f\"Forecast end date = {snapshot_end}\")\n",
    "start_time = df_raw[timestamp].min()\n",
    "end_time = snapshot_end\n",
    "\n",
    "snapshot_date_monthYear = SNAPSHOT_DATE.strftime(\"%m%d%Y\")\n",
    "EXPERIMENT_NAME = f\"{EXPERIMENT_NAME}_snap{snapshot_date_monthYear}_{FORECAST_LENGTH}{FORECAST_FREQ}\"\n",
    "print(f\"Experiment name = {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Drop null item_ids <a class=\"anchor\" id=\"drop_null_items\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templist = df_raw[item_id].unique()\n",
    "print(f\"Number unique items: {len(templist)}\")\n",
    "print(f\"Number nulls: {pd.isnull(templist).sum()}\")\n",
    "\n",
    "if len(templist) < 20:\n",
    "    print(templist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the null item_ids, if any exist\n",
    "if pd.isnull(templist).sum() > 0:\n",
    "    print(df_raw.shape)\n",
    "    df_raw = df_raw.loc[(~df_raw[item_id].isna()), :].copy()\n",
    "    print(df_raw.shape)\n",
    "    print(len(df_raw[item_id].unique()))\n",
    "else:\n",
    "    print(\"No missing item_ids found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Drop null timestamps <a class=\"anchor\" id=\"drop_null_times\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null timestamps\n",
    "templist = df_raw.loc[(df_raw[timestamp].isna()), :].shape[0]\n",
    "print(f\"Number nulls: {templist}\")\n",
    "\n",
    "if (templist < 10) & (templist > 0) :\n",
    "    print(df_raw.loc[(df_raw[timestamp].isna()), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the null quantities and dates\n",
    "if templist > 0:\n",
    "    print(df_raw.shape)\n",
    "    df_raw = df_raw.loc[(~df_raw[timestamp].isna()), :].copy()\n",
    "    print(df_raw.shape)\n",
    "    print(df_raw['timestamp'].isna().sum())\n",
    "else:\n",
    "    print(\"No null timestamps found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Inspect (and treat) extremes <a class=\"anchor\" id=\"treat_extremes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide how many extreme values is considered unusual\n",
    "MAX_EXTREMES = 10\n",
    "EXTREME_VALUE = df_raw[target_value].quantile(0.999999)\n",
    "print(EXTREME_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect extremes\n",
    "templist = df_raw.loc[(df_raw[target_value]>=EXTREME_VALUE), :].shape[0]\n",
    "print(f\"Number extremes: {templist}\")\n",
    "\n",
    "if (templist < MAX_EXTREMES) & (templist > 0) :\n",
    "    print(df_raw.loc[(df_raw[target_value]>=EXTREME_VALUE), :].set_index([item_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Current configuration - replace extremes with mean:\", replace_extremes_with_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT OVERALL TIME SERIES - TO HELP INSPECT EXTREMES\n",
    "df_raw.plot(x=timestamp, y=target_value, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a very basic example of replacing extremes. In your implementation it should be done more carefully than this.\n",
    "\n",
    "# Replace extreme value with treated median last n months values\n",
    "months_to_use = 3\n",
    "\n",
    "if (use_location):\n",
    "    # with location_id \n",
    "    if (replace_extremes_with_mean & (templist < MAX_EXTREMES) & (templist > 0) ):\n",
    "\n",
    "        df_clean = df_raw.copy()\n",
    "\n",
    "        extremes = df_raw.loc[(df_raw[target_value]>=EXTREME_VALUE), :][[item_id, location_id if use_location else None, 'day_hour', timestamp]]\n",
    "\n",
    "        for index, keys in extremes.iterrows():\n",
    "            print (keys)\n",
    "\n",
    "            # calculate median value from last n months before the extreme\n",
    "\n",
    "            temp = df_raw.loc[((df_raw[item_id]==keys[item_id]) \n",
    "                               & (df_raw[location_id]==keys[location_id])\n",
    "                               & (df_raw['day_hour']==keys['day_hour'])\n",
    "                               & (df_raw[timestamp]<keys[timestamp])\n",
    "                               & (df_raw[timestamp]>keys[timestamp] - relativedelta(months = months_to_use))), :] \n",
    "\n",
    "            # calculate median\n",
    "            replace_extreme = temp[target_value].median() \n",
    "            print(replace_extreme)\n",
    "            # visually check if replaced value looks like median target_value\n",
    "            display(temp)\n",
    "\n",
    "            # make the replacement\n",
    "            df_clean.loc[((df_clean[item_id]==keys[item_id]) \n",
    "                               & (df_clean[location_id]==keys[location_id])\n",
    "                               & (df_raw['day_hour']==keys['day_hour'])\n",
    "                               & (df_clean[timestamp]==keys[timestamp])), target_value] = replace_extreme\n",
    "\n",
    "            print(f\"new value is {df_clean.loc[((df_clean[item_id]==keys[item_id]) & (df_clean[location_id]==keys[location_id]) & (df_clean[timestamp]==keys[timestamp])), target_value].max()}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No extreme values found or do not replace any extremes.\")\n",
    "        df_clean = df_raw.copy()     \n",
    "else:\n",
    "    # without location_id     \n",
    "    if (replace_extremes_with_mean & (templist < MAX_EXTREMES) & (templist > 0) ):\n",
    "\n",
    "        df_clean = df_raw.copy()\n",
    "\n",
    "        extremes = df_raw.loc[(df_raw[target_value]>=EXTREME_VALUE), :][[item_id, 'day_hour', timestamp]]\n",
    "\n",
    "        for index, keys in extremes.iterrows():\n",
    "            print (keys)\n",
    "\n",
    "            # calculate median value from last n months before the extreme\n",
    "\n",
    "            temp = df_raw.loc[((df_raw[item_id]==keys[item_id]) \n",
    "                               & (df_raw['day_hour']==keys['day_hour'])\n",
    "                               & (df_raw[timestamp]<keys[timestamp])\n",
    "                               & (df_raw[timestamp]>keys[timestamp] - relativedelta(months = months_to_use))), :] \n",
    "\n",
    "            # calculate median\n",
    "            replace_extreme = temp[target_value].median() \n",
    "            print(replace_extreme)\n",
    "            # visually check if replaced value looks like median target_value\n",
    "            display(temp)\n",
    "\n",
    "            # make the replacement\n",
    "            df_clean.loc[((df_clean[item_id]==keys[item_id]) \n",
    "                               & (df_raw['day_hour']==keys['day_hour'])\n",
    "                               & (df_clean[timestamp]==keys[timestamp])), target_value] = replace_extreme\n",
    "\n",
    "            print(f\"new value is {df_clean.loc[((df_clean[item_id]==keys[item_id]) & (df_clean[timestamp]==keys[timestamp])), target_value].max()}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No extreme values found or do not replace any extremes.\")\n",
    "        df_clean = df_raw.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT THE CLEAN TIME SERIES - TO HELP INSPECT EXTREMES\n",
    "df_clean.plot(x=timestamp, y=target_value, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save some memory\n",
    "del df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throws error if we lost some values\n",
    "assert original_shape[0] == df_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Optional - Round negative targets up to 0 <a class=\"anchor\" id=\"round_negatives\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check negative values\n",
    "print(df_clean.loc[(df_clean[target_value] <0), :].shape)\n",
    "df_clean.loc[(df_clean[target_value] <0), :].sort_values([timestamp, item_id]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL!!  MAKE SURE ROUNDING NEGATIVES UP TO 0 MAKES SENSE FOR YOUR USE CASE\n",
    "\n",
    "# If negative values found, round them up to 0\n",
    "if df_clean.loc[(df_clean[target_value] <0), :].shape[0] > 0:\n",
    "\n",
    "    # Check y-value before cleaning\n",
    "    print(df_clean[target_value].describe())\n",
    "\n",
    "    # default negative values in demand to 0\n",
    "    print(f\"{df_clean[target_value].lt(0).sum()} negative values will be rounded up to 0\")\n",
    "    print()\n",
    "    ts_cols = [target_value]\n",
    "\n",
    "    for c in ts_cols:\n",
    "        df_clean.loc[(df_clean[c] < 0.0), c] = 0.0\n",
    "\n",
    "    # Check y-value after cleaning\n",
    "    print(df_clean[target_value].describe())\n",
    "else:\n",
    "    print(\"No negative values found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9. Optional - Convert negative targets to nan <a class=\"anchor\" id=\"negatives_to_nan\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check negative values\n",
    "# print(df_clean.loc[(df_clean[target_value] <0), :].shape)\n",
    "# df_clean.loc[(df_clean[target_value] <0), :].sort_values([timestamp, item_id]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # CAREFUL!!  MAKE SURE CHANGING NEGATIVES TO NAN NEGATIVES MAKES SENSE FOR YOUR USE CASE\n",
    "\n",
    "# # If negative values found, round them up to 0\n",
    "# if df_clean.loc[(df_clean[target_value] <0), :].shape[0] > 0:\n",
    "\n",
    "#     # Check y-value before cleaning\n",
    "#     print(df_clean[target_value].describe())\n",
    "\n",
    "#     # default negative values in demand to 0\n",
    "#     print(f\"{df_clean[target_value].lt(0).sum()} negative values will be rounded up to 0\")\n",
    "#     print()\n",
    "#     ts_cols = [target_value]\n",
    "    \n",
    "#     print ()\n",
    "\n",
    "#     for c in ts_cols:\n",
    "#         df_clean.loc[(df_clean[c] < 0.0), c] = float('nan')\n",
    "        \n",
    "        \n",
    "\n",
    "#     # Check y-value after cleaning\n",
    "#     print(df_clean[target_value].describe())\n",
    "# else:\n",
    "#     print(\"No negative values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # throws error if we lost some values\n",
    "# assert original_shape[0] == df_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10. Aggregate at your chosen frequency <a class=\"anchor\" id=\"groupby_frequency\"></a>\n",
    "\n",
    "Below, we show an example of resampling at hourly frequency by forecast dimensions.  Modify the code to resample at other frequencies.\n",
    "\n",
    "Decide which aggregation-level makes sense for your data, which is a balance between desired aggregation and what the data-collection frequency will support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## CHECK TO SEE IF YOUR TIMESERIES DIMENSIONS ARE CORRECT\n",
    "\n",
    "# checking if there are multiple entries per item_id per timestamp per location\n",
    "df_aux = df_clean.copy().set_index(forecast_dims)\n",
    "\n",
    "duplicates = df_aux.pivot_table(index=forecast_dims, aggfunc='size')\n",
    "duplicates = pd.DataFrame( duplicates, columns=[\"NumberPerTS\"])\n",
    "\n",
    "print (duplicates[duplicates[\"NumberPerTS\"]>1].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if your timeseries dimensions are correct\n",
    "if duplicates[duplicates[\"NumberPerTS\"]>1].shape[0] > 0:\n",
    "    print(\"WARNING:  YOUR AGGREGATION ASSUMPTION THAT timestamp, item_id, location_id ARE UNIQUE IS NOT CORRECT.\")\n",
    "    print(\"Inspect df_aux where you see 'NumberPerTS' > 1\")\n",
    "else:\n",
    "    print(\"Success!  timestamp, item_id, location_id is a unique grouping of your time series.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case your assumed dimensions are not unique, code below is to explore adding a composite column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "\n",
    "# #  inspect what is happening on these repeated items\n",
    "# try:\n",
    "#     df_aux.reset_index(inplace = True)\n",
    "# except Exception as e:\n",
    "#     print (e)\n",
    "\n",
    "# test_aux = df_aux.loc[((df_aux[item_id]=='PRD-05685') & (df_aux[location_id]=='STCK-00605')\n",
    "#                       & (df_aux[timestamp]==\"2019-09-30 01:36:45\")), :]\n",
    "# test_aux.sort_values(by=[timestamp])\n",
    "\n",
    "# # Possibly extra dimension Organization Name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since location is not unique, create new fake composite column\n",
    "# df_clean['timeseries_key'] = df_clean[item_id] + '-'+ df_clean[location_id] \\\n",
    "#                                 + '-' + df_clean['Organization Name']\n",
    "# df_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you changed dimensions, re-map expected column names\n",
    "# item_id = \"timeseries_key\"\n",
    "# use_location = False\n",
    "# forecast_dims = [timestamp, item_id]\n",
    "# print(f\"forecast_dims = {forecast_dims}\")\n",
    "# dims_except_timestamp = [i for i in forecast_dims if i != timestamp]\n",
    "# print(f\"dims_except_timestamp = {dims_except_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHECK AGAIN TO SEE IF YOUR DATA AGGREGATION ASSUMPTION IS CORRECT\n",
    "\n",
    "# # checking if there are multiple entries per item_id per timestamp per location\n",
    "# df_aux = df_clean[forecast_dims + [target_value]].copy().set_index(forecast_dims)\n",
    "# df_aux.drop_duplicates(inplace=True)\n",
    "\n",
    "# duplicates = df_aux.pivot_table(index=forecast_dims, aggfunc='size')\n",
    "# duplicates = pd.DataFrame( duplicates, columns=[\"NumberPerTS\"])\n",
    "\n",
    "# # checking to see if your data aggregation is correct\n",
    "# if duplicates[duplicates[\"NumberPerTS\"]>1].shape[0] > 0:\n",
    "#     print (duplicates[duplicates[\"NumberPerTS\"]>1].head())\n",
    "#     print(\"WARNING:  YOUR AGGREGATION ASSUMPTION THAT timestamp, item_id, location_id ARE UNIQUE IS NOT CORRECT.\")\n",
    "#     print(\"Inspect df_aux where you see 'NumberPerTS' > 1\")\n",
    "# else:\n",
    "#     print(\"Success!  timestamp, item_id, location_id is a unique grouping of your time series.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"forecast_dims = {forecast_dims}\")\n",
    "df_clean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "\n",
    "# restrict columns if desired\n",
    "# df_clean = df_clean[[timestamp, item_id, \"Prod #\", location_id, \"Organization Name\", target_value]].copy()\n",
    "print(df_clean.shape)\n",
    "df_clean.drop_duplicates(inplace=True)\n",
    "print(df_clean.shape)\n",
    "\n",
    "# Put all columns besides forecast_dims that you want to keep in a dictionary of aggregations (per\n",
    "# pandas agg()):\n",
    "agg_dict = {\n",
    "    \"pickup_day_of_week\": \"first\",\n",
    "    \"day_hour\": \"first\",\n",
    "    \"trip_quantity\": \"sum\",\n",
    "    \"mean_item_loc_weekday\": \"mean\",\n",
    "    \"pickup_geolocation\": \"first\",\n",
    "    \"pickup_borough\": \"first\",\n",
    "    \"binned_max_item\": \"last\",\n",
    "}\n",
    "\n",
    "print(\"Validating agg_dict...\")\n",
    "for dim in forecast_dims:\n",
    "    if dim in agg_dict:\n",
    "        dim_agg = agg_dict[dim]\n",
    "        if (type(dim_agg) == str) or not hasattr(dim_agg, \"__iter__\"):\n",
    "            print(\n",
    "                \"Single aggregation on forecast dimension column not supported: Ignoring\\n\"\n",
    "                f\"({dim}: {dim_agg})\"\n",
    "            )\n",
    "            del agg_dict[dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS CODE BLOCK IS AN EXAMPLE OF Weekly AGGREGATION\n",
    "# g_week = local_util.dataprep.aggregate_time_series(\n",
    "#     df_clean,\n",
    "#     agg_freq=\"W\",\n",
    "#     timestamp_col=timestamp,\n",
    "#     target_col=target_value,\n",
    "#     dimension_cols=dims_except_timestamp,\n",
    "#     agg_dict=agg_dict,\n",
    "#     already_grouped=False,\n",
    "#     analyze=True,\n",
    "# )\n",
    "\n",
    "# # add new time dimension since original timestamp is not weekly\n",
    "# g_week['year_week'] = g_week[timestamp].dt.year.astype(str) + '_' \\\n",
    "#                         + g_week[timestamp].dt.isocalendar().week.astype(str)\n",
    "# display(g_week.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE BLOCK IS AN EXAMPLE OF Hourly AGGREGATION\n",
    "# Note:  The sample data shipped with notebook is ideal - all time series have 5856 data points, \n",
    "# which is squarely in the Deep Learning desired data size.\n",
    "\n",
    "g_hour = local_util.dataprep.aggregate_time_series(\n",
    "    df_clean,\n",
    "    agg_freq=\"H\",\n",
    "    timestamp_col=timestamp,\n",
    "    target_col=target_value,\n",
    "    dimension_cols=dims_except_timestamp,\n",
    "    agg_dict=agg_dict,\n",
    "    already_grouped=True,\n",
    "    analyze=True,\n",
    ")\n",
    "display(g_hour.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRY ANOTHER AGGREGATION LEVEL AND COMPARE TARGET_VALUE DISTRIBUTION SHAPES\n",
    "agg_freq = \"2H\"\n",
    "\n",
    "g_2hour = local_util.dataprep.aggregate_time_series(\n",
    "    df_clean,\n",
    "    \"2H\",\n",
    "    timestamp_col=timestamp,\n",
    "    target_col=target_value,\n",
    "    dimension_cols=dims_except_timestamp,\n",
    "    agg_dict=agg_dict,\n",
    ")\n",
    "display(g_2hour.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRY ANOTHER AGGREGATION LEVEL AND COMPARE TARGET_VALUE DISTRIBUTION SHAPES\n",
    "agg_freq = \"4H\"\n",
    "\n",
    "g_4hour = local_util.dataprep.aggregate_time_series(\n",
    "    df_clean,\n",
    "    \"4H\",\n",
    "    timestamp_col=timestamp,\n",
    "    target_col=target_value,\n",
    "    dimension_cols=dims_except_timestamp,\n",
    "    agg_dict=agg_dict,\n",
    ")\n",
    "display(g_4hour.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b> Select the aggregation-level to keep, based on results above.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE THE GROUPING YOU SELECTED ABOVE\n",
    "df = g_hour.copy()\n",
    "\n",
    "# Delete no-longer-needed aggregations to save memory:\n",
    "del df_aux\n",
    "del g_hour\n",
    "del g_2hour\n",
    "del g_4hour\n",
    "\n",
    "print(df.shape, df_clean.shape)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11. Typical retail scenarios: Find top-moving items <a class=\"anchor\" id=\"top_moving_items\"></a>\n",
    "\n",
    "Next, we want to drill down and visualize some individual item time series.  Typically customers have \"catalog-type\" data, where only the top 20% of their items are top-movers; the rest of the 80% of items are not top-movers.  For visualization, we want to select automatically some of the top-moving items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"Calculating per item{'+location' if use_location else ''} velocities\")\n",
    "top_movers, slow_movers = local_util.analysis.get_top_moving_items(\n",
    "    df,\n",
    "    timestamp,\n",
    "    target_value,\n",
    "    item_id,\n",
    "    location_id if use_location else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_random = 5\n",
    "print(f\"Selecting {n_random} random top-moving series for plotting:\")\n",
    "random_series = top_movers.sample(\n",
    "    n_random,\n",
    "    random_state=42,\n",
    ").index.to_frame().reset_index(drop=True)\n",
    "random_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you could explicitly specify some dimension combinations you'd like to explore:\n",
    "\n",
    "# random_series = pd.DataFrame({\n",
    "#     item_id: [\"79\", \"135\"],\n",
    "#     location_id: [\"here\", \"there\"],\n",
    "# })\n",
    "# random_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step12. Visualize time series <a class=\"anchor\" id=\"visualize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = local_util.dataprep.select_by_df(df, random_series)\n",
    "\n",
    "df_plot.set_index(timestamp, inplace=True)\n",
    "df_plot.head(2)\n",
    "\n",
    "local_util.plotting.make_plots(\n",
    "    df_plot,\n",
    "    random_series,\n",
    "    target_value,\n",
    "    \"Hourly quantity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13. Split train/test data  <a class=\"anchor\" id=\"split_train_test\"></a> \n",
    "\n",
    "In forecasting, \"train\" data is until a last-train date, sometimes called the forecast snapshot date.  \n",
    "<ul>\n",
    "    <li>Train data includes all data up to your last-train date. </li>\n",
    "    <li>Test data includes dates after your last-train date through end of desired forecast horizon.</li>\n",
    "    <li>Validation data might exist for part or maybe all of the desired forecast horizon. </li>\n",
    "    <li>TTS timestamps should start and end with Train data. </li>\n",
    "    <li>RTS timestamps should start with Train data and extend out past end of TTS to end of the desired forecast horizon.</li>\n",
    "    </ul>\n",
    "    \n",
    "For model generalization, all processing from here on out will only be done on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Horizon is number of time steps out in the future you want to predict\n",
    "# Time steps are defined in the time frequency you specified in Step 5 Aggregate\n",
    "\n",
    "# Example if aggregation was hourly, then forecast length=168 means forecast horizon of 7 days or 7*24=168 hours\n",
    "print(f\"Forecast horizon = {FORECAST_LENGTH}\") # = 12\n",
    "print(f\"Forecast unit of frequency = {AF_freq_to_dateutil_freq[FORECAST_FREQ]}\") # = 30\n",
    "print(f\"Forecast start date = {SNAPSHOT_DATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train data as all except last FORECAST_HORIZON length\n",
    "start_time = df[timestamp].min()\n",
    "end_time = snapshot_end\n",
    "start_time_test = SNAPSHOT_DATE\n",
    "\n",
    "print(f\"start_time = {start_time}\")\n",
    "print(f\"end_time_train = {end_time_train}\")\n",
    "print(f\"start_time_test = {start_time_test}\")\n",
    "print(f\"end_time = {snapshot_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_future_RTS_with_unknown_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_future_RTS_with_unknown_data:\n",
    "    # Create train data as all data => this means RTS will extend into unknown future\n",
    "    print(\"using all known data for training\")\n",
    "    train_df = df.copy()\n",
    "else:\n",
    "    # Create train subset with hold-out of length FORECAST_LENGTH\n",
    "    print(\"using hold-out with train data\")\n",
    "    train_df = df.copy()\n",
    "    train_df = train_df.loc[(train_df[timestamp] <= end_time_train), :]\n",
    "\n",
    "# check you did the right thing\n",
    "print(f\"start_time = {start_time}\")\n",
    "print(f\"end_time: {end_time}\")\n",
    "print()\n",
    "print(f\"start_time_train = {train_df[timestamp].min()}\")\n",
    "print(f\"end_time_train = {train_df[timestamp].max()}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR CHECK: DO YOU HAVE ENOUGH HISTORICAL DATA POINTS TO SUPPORT DESIRED FORECAST HORIZON?\n",
    "\n",
    "# calculate number data points in train data\n",
    "num_data_points = train_df.groupby(dims_except_timestamp).nunique()[timestamp].mean()\n",
    "print(f\"1/3 training data points: {np.round(num_data_points/3,0)}\")\n",
    "\n",
    "# Amazon Forecast length of forecasts can be 500 data points and 1/3 target time series dataset len\n",
    "if ((FORECAST_LENGTH < 500) & (FORECAST_LENGTH <= np.round(num_data_points/3,0))):\n",
    "    print(\"\".join((\n",
    "        f\"Success, forecast horizon {FORECAST_LENGTH} is shorter than 500 data points and less \",\n",
    "        \"than 1/3 of the historical training data length.\",\n",
    "    )))\n",
    "else:\n",
    "    raise ValueError(\"\".join((\n",
    "        f\"Error, forecast horizon {FORECAST_LENGTH} is too long. Must be fewer than 500 data \",\n",
    "        \"points and less than 1/3 of the historical training data length.\",\n",
    "    )))\n",
    "    \n",
    "# If you have too few data points, return to step above and choose smaller time granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14. Prepare and Save Target Time Series (TTS) <a class=\"anchor\" id=\"TTS\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_future_RTS_with_unknown_data)\n",
    "print(f\"forecast_dims: {forecast_dims}\")\n",
    "print(f\"geolocation: {geolocation}\")\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assemble TTS required columns\n",
    "\n",
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "if geolocation is not None:\n",
    "    print(\"Preparing TTS with geolocation data\")\n",
    "    # restrict train data to just tts columns\n",
    "    tts = train_df[forecast_dims + [geolocation, target_value]].copy()\n",
    "    tts = tts.groupby(forecast_dims+[geolocation])[[target_value]].sum()\n",
    "else:\n",
    "    print(\"Running without geolocation data\")\n",
    "    # restrict train data to just tts columns\n",
    "    tts = train_df[forecast_dims + [target_value]].copy()\n",
    "    tts = train_df[[timestamp, item_id, target_value]].copy()\n",
    "    tts = tts.groupby(forecast_dims)[[target_value]].sum()\n",
    "    \n",
    "tts.reset_index(inplace=True)\n",
    "print(f\"start date = {tts[timestamp].min()}\")\n",
    "print(f\"end date = {tts[timestamp].max()}\")\n",
    "\n",
    "# check it\n",
    "print(tts.shape)\n",
    "print(tts.dtypes)\n",
    "tts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check format of geolocation column\n",
    "# tts[geolocation].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optional - convert target_value to integer if this is the last step for TTS. </b>\n",
    "\n",
    "Note: Currently in Amazon Forecast, if you declare target_value is integer in the schema, but you have any decimals in your numbers, you will get an error.\n",
    "\n",
    "Make sure you really see integers in the code below, if you want integers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new pandas Integer type\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "# TODO: turn this into a function\n",
    "\n",
    "if target_to_integer:\n",
    "    try:\n",
    "        tts[target_value] = tts[target_value].astype(int)\n",
    "        print(\"Success! Converted to numpy integer\")\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print(\"Trying pandas nullable Integer type instead of numpy integer type...\")\n",
    "        try:\n",
    "            tts[target_value] = tts[target_value].astype('Int64', errors='ignore')\n",
    "            print(\"Success! converted to pandas integer\")\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "elif tts[target_value].dtype == 'object':\n",
    "    # convert to float\n",
    "    tts[target_value] = tts[target_value].astype(np.float32)\n",
    "elif tts[target_value].dtype != 'object':\n",
    "    # do nothing\n",
    "    print(\"target_value is already a float\")\n",
    "    \n",
    "print(tts.dtypes)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15. Remove time series with no target values at all<a class=\"anchor\" id=\"TTS_remove_all0\"></a>\n",
    "In case there are time series which are only 0's, may as well remove them, since their forecast should be all 0's too.  Another reason to remove these time series is they could bias the overall forecast toward 0, when that's not what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tts.shape)\n",
    "tts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if sum of all sales is 0\n",
    "g = tts.groupby(dims_except_timestamp).sum()\n",
    "g.fillna(0, inplace=True)\n",
    "skus_with_no_sales_in_warehouse = g[g[target_value] == 0].copy()\n",
    "\n",
    "# drop extra columns for cleaner merge\n",
    "skus_with_no_sales_in_warehouse.reset_index(inplace=True)\n",
    "skus_with_no_sales_in_warehouse = skus_with_no_sales_in_warehouse.iloc[:, 0:1]\n",
    "skus_with_no_sales_in_warehouse.drop_duplicates(inplace=True)\n",
    "display (skus_with_no_sales_in_warehouse.head(2))\n",
    "\n",
    "if skus_with_no_sales_in_warehouse.shape[0] > 0:\n",
    "        \n",
    "    # https://stackoverflow.com/questions/32676027/how-to-do-df1-not-df2-dataframe-merge-in-pandas\n",
    "    tts_copy = tts.merge(skus_with_no_sales_in_warehouse, how='left', on=dims_except_timestamp, indicator=True) \\\n",
    "               .query(\"_merge=='left_only'\") \\\n",
    "               .drop('_merge',1)\n",
    "\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    display(tts_copy.sample(5))\n",
    "\n",
    "else:\n",
    "    print(\"No time series found with only 0's.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really drop skus with only 0's\n",
    "\n",
    "if skus_with_no_sales_in_warehouse.shape[0] > 0:\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    tts = tts_copy.copy()\n",
    "    del (tts_copy)\n",
    "    \n",
    "    # keep track of dropped dimensions and reason why dropped\n",
    "    skus_with_no_sales_in_warehouse = skus_with_no_sales_in_warehouse[dims_except_timestamp].copy()\n",
    "    \n",
    "    # save the reason\n",
    "    skus_with_no_sales_in_warehouse['reason'] = \"All 0's\"\n",
    "    display(skus_with_no_sales_in_warehouse.head(2))\n",
    "    \n",
    "    print(tts.shape)\n",
    "    display(tts.sample(5))\n",
    "\n",
    "else:\n",
    "    print(\"Didn't drop anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16. Remove time series with end of life<a class=\"anchor\" id=\"TTS_remove_end_of_life\"></a>\n",
    "\n",
    "Check if time series have any data in last 6 months and more than 5 data points, since 5 data points is minimum for Amazon Forecast to generate forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tts.shape)\n",
    "tts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define end of life = No sales in the last 6 months\n",
    "\n",
    "# first get df of only last 6 months\n",
    "time_threshold = end_time - datetime.timedelta(6*30) \n",
    "\n",
    "# check if sum of sales last 6 months is 0\n",
    "tts_aux = tts[tts[timestamp] >= time_threshold].copy()\n",
    "g = tts_aux.groupby(dims_except_timestamp).sum()\n",
    "g.fillna(0, inplace=True)\n",
    "skus_with_end_of_life = g[g[target_value] == 0].copy()\n",
    "\n",
    "# drop extra columns for cleaner merge\n",
    "skus_with_end_of_life.reset_index(inplace=True)\n",
    "skus_with_end_of_life = skus_with_end_of_life.iloc[:, 0:1]\n",
    "skus_with_end_of_life.drop_duplicates(inplace=True)\n",
    "display (skus_with_end_of_life.head(2))\n",
    "\n",
    "if skus_with_end_of_life.shape[0] > 0:\n",
    "\n",
    "    # https://stackoverflow.com/questions/32676027/how-to-do-df1-not-df2-dataframe-merge-in-pandas\n",
    "    tts_copy = tts.merge(skus_with_end_of_life, how='left', on=dims_except_timestamp, indicator=True) \\\n",
    "               .query(\"_merge=='left_only'\") \\\n",
    "               .drop('_merge',1)\n",
    "\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    display(tts_copy.sample(5))\n",
    "\n",
    "else:\n",
    "    print(\"No time series found with end of life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really drop the skus with end of life\n",
    "\n",
    "if skus_with_end_of_life.shape[0] > 0:\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    tts = tts_copy.copy()\n",
    "    del (tts_copy)\n",
    "    display(tts.dtypes)\n",
    "    \n",
    "    # keep track of dropped dimensions and reason\n",
    "    skus_with_end_of_life = skus_with_end_of_life[dims_except_timestamp].copy()\n",
    "    skus_with_end_of_life['reason'] = \"end of life\"\n",
    "    display(skus_with_end_of_life.head(2))\n",
    "    \n",
    "    print(tts.shape)\n",
    "    display(tts.sample(5))\n",
    "    \n",
    "else:\n",
    "    print(\"Didn't drop anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17. Remove time series with fewer than 5 data points<a class=\"anchor\" id=\"TTS_remove_too_few_data_points\"></a>\n",
    "\n",
    "Minimum number of data points is 5 data points to make a forecast.  <br>\n",
    "\n",
    "**Note: special consideration for cold-start or new product introductions**.  For best results, do not include new items in your training data.  However, do include new items in your inference data.  Notice that there is a system constraint such that at least 5 data points need to exist for each time series. Therefore, for the item that has less than 5 observations, be sure that item's target_value is encoded as float and fill explicitly with \"NaN\".  Also note: Cold-start forecasting only works if new items are tied to items with longer histories through Item Metadata. \n",
    "\n",
    "Run this to remove rows with <5 values (not explicitly \"NaN\") manually and save the list of time series with too few data points for your own reference.  Otherwise if you skip this section, Forecast will automatically drop (silently) all time series with fewer than 5 data points, since that is too few to make a good forecast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tts.shape)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Replacing '0's with null</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_all_zeroes_with_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Null-value filling, if any\n",
    "\n",
    "# special case:  replace 0s with nulls\n",
    "if (replace_all_zeroes_with_null):\n",
    "    print(tts.shape)\n",
    "    print(tts[target_value].describe())\n",
    "    if target_to_integer:\n",
    "        tts.loc[(tts[target_value]==0), target_value] = pd.NA\n",
    "    else:\n",
    "        tts.loc[(tts[target_value]==0), target_value] = np.nan\n",
    "    print ()\n",
    "    print(tts.shape)\n",
    "    print(tts[target_value].describe())\n",
    "else:\n",
    "    tts.loc[:, target_value].fillna(0, inplace=True)\n",
    "    print(\"No null-filling required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check per time series if count of data points is at least 5\n",
    "g = tts.groupby(dims_except_timestamp).count()\n",
    "skus_with_too_few_sales = g[g[target_value] < 5].copy()\n",
    "\n",
    "# drop extra columns for cleaner merge\n",
    "skus_with_too_few_sales.reset_index(inplace=True)\n",
    "skus_with_too_few_sales = skus_with_too_few_sales.iloc[:, 0:1]\n",
    "skus_with_too_few_sales.drop_duplicates(inplace=True)\n",
    "display (skus_with_too_few_sales.head(2))\n",
    "\n",
    "if skus_with_too_few_sales.shape[0] > 0:\n",
    "\n",
    "    # https://stackoverflow.com/questions/32676027/how-to-do-df1-not-df2-dataframe-merge-in-pandas\n",
    "    tts_copy = tts.merge(skus_with_too_few_sales, how='left', on=dims_except_timestamp, indicator=True) \\\n",
    "               .query(\"_merge=='left_only'\") \\\n",
    "               .drop('_merge',1)\n",
    "\n",
    "    print(\"TTS if you dropped items with too few data points\")\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    display(tts_copy.sample(5))\n",
    "\n",
    "else:\n",
    "    print(\"No time series found with fewer than 5 datapoints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really drop skus with too few data points, only if more than a handful found\n",
    "\n",
    "if skus_with_too_few_sales.shape[0] > 0:\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    tts = tts_copy.copy()\n",
    "    del (tts_copy)\n",
    "    \n",
    "    # keep track of dropped dimensions and reason why dropped\n",
    "    skus_with_too_few_sales = skus_with_too_few_sales[dims_except_timestamp].copy()\n",
    "    skus_with_too_few_sales['reason'] = \"Fewer than 5 datapoints\"\n",
    "    display(skus_with_too_few_sales.head(2))\n",
    "    \n",
    "    print(tts.shape)\n",
    "    display(tts.sample(5))\n",
    "    \n",
    "else:\n",
    "    print(\"Didn't drop anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Keep track of dropped time series and reason why they were dropped. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skus_with_too_few_sales.shape[0] > 0:\n",
    "    dropped_dims = skus_with_too_few_sales.append([skus_with_no_sales_in_warehouse\n",
    "                                                       , skus_with_end_of_life])\n",
    "    print(f\"unique ts dropped = {dropped_dims.shape[0]}\")\n",
    "    print(f\"unique ts fewer than 5 data points = {skus_with_too_few_sales.shape[0]}\")\n",
    "    print(f\"unique ts with all 0s = {skus_with_no_sales_in_warehouse.shape[0]}\")\n",
    "    print(f\"unique ts with end of life = {skus_with_end_of_life.shape[0]}\")\n",
    "    display(dropped_dims.reason.value_counts(dropna=False, normalize=True))\n",
    "    display(dropped_dims.sample(1))\n",
    "else:\n",
    "    print(\"Didn't drop anything\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of dropped skus and reasons for reference and to check if data can be fixed\n",
    "\n",
    "if skus_with_too_few_sales.shape[0] > 2:\n",
    "    # save all the dropped dimensions fields\n",
    "    local_file = \"data/dropped_fields.csv\"\n",
    "    # Save merged file locally\n",
    "    dropped_dims.to_csv(local_file, header=True, index=False)\n",
    "\n",
    "    key = f\"{prefix}/v{DATA_VERSION}/dropped_{EXPERIMENT_NAME}.csv\"\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optional - convert target_value to integer if this is last step for TTS. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tts.shape)\n",
    "display(tts.dtypes)\n",
    "tts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new pandas Integer type\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "\n",
    "if target_to_integer:\n",
    "    try:\n",
    "        tts[target_value] = tts[target_value].fillna(0).astype(int)\n",
    "        print(\"Success! Converted to np.integer type\")\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print(\"Trying pandas nullable Integer type instead of numpy integer type...\")\n",
    "        try:\n",
    "            tts[target_value] = tts[target_value].astype('Int64', errors='ignore')\n",
    "            print(\"Success! Converted to nullable pd.integer type\")\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "elif tts[target_value].dtype == 'object':\n",
    "    # convert to float\n",
    "    tts[target_value] = tts[target_value].astype(np.float32)\n",
    "elif tts[target_value].dtype != 'object':\n",
    "    # do nothing\n",
    "    print(\"target_value is already a float\")\n",
    "    \n",
    "print(tts.dtypes)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Optional - replace 0's with nulls </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_all_zeroes_with_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replace_all_zeroes_with_null:\n",
    "    tts.loc[(tts[target_value]==0), target_value] = pd.NA\n",
    "    \n",
    "print(tts[target_value].describe())\n",
    "print(tts.dtypes)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one last check...\n",
    "print(tts.shape)\n",
    "tts.drop_duplicates(inplace=True)\n",
    "print(tts.shape)\n",
    "print(tts[timestamp].min())\n",
    "print(tts[timestamp].max())\n",
    "# check for nulls\n",
    "print(tts.isnull().sum())\n",
    "print(tts.dtypes)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check input numbers of time series\n",
    "if skus_with_too_few_sales.shape[0] > 0:\n",
    "    dropped = dropped_dims.groupby(dims_except_timestamp).first().shape[0]\n",
    "    display(dropped)\n",
    "    # check\n",
    "    assert (train_df.groupby(dims_except_timestamp).first().shape[0] \\\n",
    "            == (tts.groupby(dims_except_timestamp).first().shape[0] + dropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts to S3\n",
    "local_file = \"data/tts.csv\"\n",
    "# Save merged file locally\n",
    "tts.to_csv(local_file, header=False, index=False)\n",
    "print(f\"Saved TTS locally to {local_file}\")\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)\n",
    "print(f\"Uploaded TTS to s3://{bucket_name}/{key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18. Optional - Assemble and save TTS_sparse, TTS_dense <a class=\"anchor\" id=\"TTS-dense_sparse\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_except_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense, sparse = local_util.analysis.analyze_lengths_and_sparsity(\n",
    "    tts,\n",
    "    agg_freq=\"H\",\n",
    "    target_col=target_value,\n",
    "    forecast_dims=dims_except_timestamp,\n",
    "    dense_threshold_quantile=0.75,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(dense)} dense timeseries\")\n",
    "print(\"Datapoint count of densest items/timeseries:\")\n",
    "display(dense.head())\n",
    "print(f\"\\nFound {len(sparse)} sparse timeseries\")\n",
    "print(\"Datapoint count of sparsest items/timeseries:\")\n",
    "display(sparse.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot check some sparse time-series:\n",
    "local_util.plotting.make_plots(\n",
    "    df,\n",
    "    sparse.tail(3).index.to_frame(),\n",
    "    target_value,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sparse dimensions\n",
    "local_file = \"data/sparse_fields.csv\"\n",
    "# Save merged file locally\n",
    "sparse.reset_index().to_csv(local_file, header=False, index=False)\n",
    "print(f\"Saved sparse dimensions locally to {local_file}\")\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/sparse_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)\n",
    "print(f\"Uploaded to s3://{bucket_name}/{key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dense dimensions\n",
    "local_file = \"data/dense_fields.csv\"\n",
    "# Save merged file locally\n",
    "dense.reset_index().to_csv(local_file, header=False, index=False)\n",
    "print(f\"Saved dense dimensions locally to {local_file}\")\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/dense_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)\n",
    "print(f\"Uploaded to s3://{bucket_name}/{key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_dense = local_util.dataprep.select_by_df(tts, dense.index.to_frame()).copy()\n",
    "print(tts_dense.shape, tts.shape)\n",
    "tts_dense.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_dense to S3\n",
    "local_file = \"data/tts_dense.csv\"\n",
    "# Save merged file locally\n",
    "tts_dense.to_csv(local_file, header=False, index=False)\n",
    "print(f\"Saved dense-only TTS locally to {local_file}\")\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_dense_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)\n",
    "print(f\"Uploaded to s3://{bucket_name}/{key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some memory:\n",
    "del tts_dense\n",
    "del dense\n",
    "# We'll use `sparse` in an optional section later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19. Optional - Assemble and save tts.top, tts.slow <a class=\"anchor\" id=\"TTS_top\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_top = local_util.dataprep.select_by_df(tts, top_movers.index.to_frame())\n",
    "\n",
    "print(f\"Selected {tts_top.shape} from {tts.shape}\")\n",
    "num_top_items = tts_top.groupby(dims_except_timestamp).first().shape[0]\n",
    "print(f\"Number top items = {num_top_items}\")\n",
    "tts_top.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_top to S3\n",
    "local_file = \"data/tts_top.csv\"\n",
    "# Save merged file locally\n",
    "tts_top.to_csv(local_file, header=False, index=False)\n",
    "print(f\"Saved top-moving TTS locally to {local_file}\")\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_top_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)\n",
    "print(f\"Uploaded to s3://{bucket_name}/{key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_slow = local_util.dataprep.select_by_df(tts, slow_movers.index.to_frame())\n",
    "\n",
    "print(f\"Selected {tts_slow.shape} from {tts.shape}\")\n",
    "num_slow_items = tts_slow.groupby(dims_except_timestamp).first().shape[0]\n",
    "print(f\"Number slow items = {num_slow_items}\")\n",
    "tts_slow.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_slow to S3\n",
    "local_file = \"data/tts_slow.csv\"\n",
    "# Save merged file locally\n",
    "tts_slow.to_csv(local_file, header=False, index=False)\n",
    "print(f\"Saved slow-moving TTS locally to {local_file}\")\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_slow_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)\n",
    "print(f\"Uploaded to s3://{bucket_name}/{key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some memory:\n",
    "del tts_top\n",
    "del tts_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20. Prepare and save RTS (if any) <a class=\"anchor\" id=\"RTS\"></a>\n",
    "\n",
    "Make sure RTS does not have any missing values, even if RTS extends into future. <br>\n",
    "Trick:  create dataframe without any missing values using cross-join, faster than resample technique. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get memory allocation error in merges below, try overriding default value 0 to 1 for overcommit\n",
    "# see https://www.kernel.org/doc/Documentation/vm/overcommit-accounting\n",
    "# Next 2 commands - open new terminal and do these directly in terminal\n",
    "# !sudo -i \n",
    "# !echo 1 > /proc/sys/vm/overcommit_memory\n",
    "!cat /proc/sys/vm/overcommit_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, delete all local files to free up disk space\n",
    "# !rm data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_times = pd.DataFrame({\n",
    "    timestamp: pd.date_range(start=start_time, end=end_time, freq=FORECAST_FREQ),\n",
    "})\n",
    "\n",
    "# # Create other time-related columns if you need them in RTS\n",
    "# all_times['year_week'] = all_times[timestamp].dt.year.astype(str) + '_' + all_times[timestamp].dt.month.astype(str)\n",
    "\n",
    "print(f\"Number of data points: {len(all_times)}\")\n",
    "print(f\"Start date = {all_times[timestamp].min()}\")\n",
    "print(f\"End date = {all_times[timestamp].max()}\")\n",
    "print(all_times.dtypes)\n",
    "print(all_times.isna().sum())\n",
    "print(all_times.shape)\n",
    "all_times.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create master template of all possible locations and items\n",
    "try:\n",
    "    print(f\"found geolocation {geolocation}\")\n",
    "    items = df.groupby([item_id, geolocation])[[item_id, geolocation]].min()\n",
    "except:\n",
    "    items = pd.DataFrame(list(df[item_id].unique()))\n",
    "    items.columns = [item_id]\n",
    "# print(items.head(2))\n",
    "\n",
    "if use_location:\n",
    "    locations = pd.DataFrame(list(df[location_id].unique()))\n",
    "    locations.columns = [location_id]\n",
    "#     print(locations.head(2))\n",
    "    locations['key'] = 1\n",
    "    items['key'] = 1\n",
    "    # Do the cross-join\n",
    "    master_records = locations.merge(items, on ='key').drop(\"key\", 1) \n",
    "    print(master_records.shape, items.shape, locations.shape)\n",
    "    num_locs = len(master_records[location_id].value_counts())\n",
    "    print(f\"num locations = {num_locs}\")\n",
    "else:\n",
    "    master_records = items.copy()\n",
    "    print(master_records.shape, items.shape)\n",
    "\n",
    "# check you did the right thing\n",
    "num_items = len(master_records[item_id].value_counts())\n",
    "print(f\"num items = {num_items}\")\n",
    "master_records.tail()\n",
    "\n",
    "# CPU times: user 688 ms, sys: 66.7 ms, total: 755 ms\n",
    "# Wall time: 752 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cross-join to create master template of all possible locations and items and times\n",
    "all_times['key'] = \"1\"\n",
    "master_records['key'] = \"1\"\n",
    "all_times.set_index('key', inplace=True)\n",
    "master_records.set_index('key', inplace=True)\n",
    "\n",
    "# Do the cross-join\n",
    "print(\"doing the merge...\")\n",
    "full_history = master_records.merge(all_times, how=\"outer\", left_index=True, right_index=True)\n",
    "print(\"done w/ merge...\")\n",
    "full_history.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# make sure you don't have any nulls\n",
    "print(full_history.shape)\n",
    "print(\"checking nulls...\")\n",
    "print(full_history.isna().sum())\n",
    "full_history.tail()\n",
    "\n",
    "# CPU times: user 265 ms, sys: 27.6 ms, total: 293 ms\n",
    "# Wall time: 290 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small df of target_values - to merge later using dask\n",
    "temp_target = df[forecast_dims + [target_value]].copy()\n",
    "# add key for faster join\n",
    "temp_target[\"ts_key\"] = temp_target[timestamp].astype(str).str.cat(temp_target[dims_except_timestamp], sep=\"-\")\n",
    "temp_target = temp_target.groupby('ts_key').sum()\n",
    "# temp_target.drop(forecast_dims, inplace=True, axis=1)\n",
    "# temp_target.set_index('ts_key', inplace=True)\n",
    "print(temp_target.shape, df.shape)\n",
    "display(temp_target.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallelization for faster merge**\n",
    "\n",
    "In the [local_util/distributed.py](local_util/distributed.py) utilities, we use [Dask](https://docs.dask.org/en/latest/) to support parallelizing the merge for large datasets where this may be faster.\n",
    "\n",
    "For more information, check out the documentation in the source code or just run `help(local_util.distributed)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# CPU times: user 5.54 s, sys: 300 ms, total: 5.84 s\n",
    "# Wall time: 5.91 s\n",
    "\n",
    "full_history[\"ts_key\"] = full_history[timestamp].astype(str).str.cat(\n",
    "    full_history[dims_except_timestamp],\n",
    "    sep=\"-\",\n",
    ")\n",
    "print(\"full_history:\")\n",
    "display(full_history.head(2))\n",
    "\n",
    "num_partitions = local_util.distributed.suggest_num_dask_partitions(\n",
    "    dims_except_timestamp,\n",
    "    num_items,\n",
    ")\n",
    "\n",
    "temp = local_util.distributed.merge(\n",
    "    full_history,\n",
    "    temp_target,\n",
    "    num_dask_partitions=num_items if use_location else 1,\n",
    "    # use_dask_if_available=False,  # Can try un-commenting this if you have memory issues\n",
    "    how=\"left\",\n",
    "    left_on=\"ts_key\",\n",
    "    right_index=True,\n",
    ").drop(columns=[\"ts_key\"])\n",
    "\n",
    "print(\"\\nJOIN RESULT:\")\n",
    "print(type(temp), temp.shape)\n",
    "display(temp.head(2))\n",
    "\n",
    "print(\"Statistics should be unchanged after merge:\")\n",
    "display(pd.concat(\n",
    "    { \"Original temp_target\": temp_target.describe(), \"Merged\": temp.describe()},\n",
    "    axis=1,\n",
    "    names=(\"DataFrame\", \"Column\"),\n",
    "))\n",
    "\n",
    "print(\"Missing values after merge:\")\n",
    "print(temp.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful!!\n",
    "# Really replace full_history with merged values, if the merge results above look OK\n",
    "full_history = temp\n",
    "del temp, temp_target\n",
    "full_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "\n",
    "# Create other time-related columns if you need them in RTS\n",
    "\n",
    "# Candidate variables for weekly data\n",
    "# full_history['month'] = full_history[timestamp].dt.month.astype(str)\n",
    "# full_history['year'] = full_history[timestamp].dt.year.astype(str)\n",
    "# full_history['quarter'] = full_history[timestamp].dt.quarter.astype(str)\n",
    "# full_history['year_month'] = full_history['year'] + '_' + full_history['month']\n",
    "# full_history['year_quarter'] = full_history['year'] + '_' + full_history['quarter']\n",
    "\n",
    "# Candidate variables for hourly data\n",
    "full_history['day_of_week'] = full_history[timestamp].dt.day_name().astype(str)\n",
    "full_history['hour_of_day'] = full_history[timestamp].dt.hour.astype(str)\n",
    "full_history['day_hour_name'] = full_history['day_of_week'] + \"_\" + full_history['hour_of_day']\n",
    "full_history['weekend_flag'] = full_history[timestamp].dt.dayofweek\n",
    "full_history['weekend_flag'] = (full_history['weekend_flag'] >= 5).astype(int)\n",
    "full_history['is_sun_mon'] = 0\n",
    "full_history.loc[((full_history.day_of_week==\"Sunday\") | (full_history.day_of_week==\"Monday\")), 'is_sun_mon'] = 1\n",
    "\n",
    "print(full_history.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - create feature from target_value that is sometimes useful\n",
    "\n",
    "# # calculate mean sales per item per year\n",
    "# TODO: add normalization here\n",
    "\n",
    "# temp_year_item = train_df[['year', item_id, target_value]].copy()\n",
    "# temp_year_item.year = temp_year_item.year.astype(str)\n",
    "# temp_year_item = temp_year_item.groupby(['year', item_id]).mean()\n",
    "# temp_year_item.reset_index(inplace=True)\n",
    "# temp_year_item.rename(columns={target_value:\"count_year_item\"}, inplace=True)\n",
    "# print(temp_year_item.dtypes)\n",
    "# temp_year_item.sample(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge in year-item trend\n",
    "\n",
    "# temp2 = full_history.copy()\n",
    "# # temp.drop(\"count_day_loc_item\", inplace=True, axis=1)\n",
    "# print(temp2.shape)\n",
    "# temp = temp2.merge(temp_year_item, how=\"left\", on=[\"year\", item_id])\n",
    "# print(temp.shape, temp_year_item.shape)\n",
    "\n",
    "# # check nulls\n",
    "# print(temp.isna().sum())\n",
    "# temp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Careful!!\n",
    "# # Really replace full_history with merged values\n",
    "# full_history = temp.copy()\n",
    "# full_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom-in time slice so you can see patterns\n",
    "df_plot = local_util.dataprep.select_by_df(full_history, random_series)\n",
    "df_plot = df_plot.loc[\n",
    "    (df_plot[timestamp] > \"2020-01-10\")\n",
    "    & (df_plot[timestamp] < end_time_train)\n",
    "]\n",
    "print(df_plot.shape, full_history.shape)\n",
    "df_plot = df_plot.groupby([timestamp]).sum()\n",
    "df_plot.reset_index(inplace=True)\n",
    "df_plot.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check: target_value distribution in full dataframe looks same as original\n",
    "df_plot[target_value].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE HOURLY RTS\n",
    "\n",
    "# Visualize candidate RTS variables\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "df_plot.plot(x=timestamp, y=target_value, ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "df_plot.plot(x=timestamp, y='weekend_flag', color='red', alpha=0.3, ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE HOURLY RTS\n",
    "\n",
    "# Visualize candidate RTS variables is_sun_mon\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "df_plot.plot(x=timestamp, y=target_value, ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "df_plot.plot(x=timestamp, y='is_sun_mon', color='red', alpha=0.3, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like lowest taxis rides are a combination of day and hour that seems to matter, not just day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_history.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE HOURLY RTS\n",
    "\n",
    "# Assemble RTS - include whatever columns you finally decide\n",
    "if geolocation is not None:\n",
    "    rts = full_history[forecast_dims + [geolocation] + ['day_hour_name']].copy()\n",
    "else:\n",
    "    rts = full_history[forecast_dims + ['day_hour_name']].copy()\n",
    "\n",
    "print(rts.shape)\n",
    "print(rts.isnull().sum())\n",
    "print(f\"rts start: {rts[timestamp].min()}\")\n",
    "print(f\"rts end: {rts[timestamp].max()}\")\n",
    "rts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rts to S3\n",
    "local_file = \"data/rts.csv\"\n",
    "# Save merged file locally\n",
    "rts.to_csv(local_file, header=False, index=False)\n",
    "print(f\"Saved RTS locally to {local_file}\")\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/{EXPERIMENT_NAME}.related.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)\n",
    "print(f\"Uploaded to s3://{bucket_name}/{key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_times\n",
    "del master_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 21. Classify Time Series <a class=\"anchor\" id=\"Classify\"></a>\n",
    "Using definitions given here:  https://frepple.com/blog/demand-classification/\n",
    "Original article: https://robjhyndman.com/papers/idcat.pdf <br>\n",
    "\n",
    "Idea:  Based on demand patterns, time series can be classified into one of 4 classes:  Smooth, Intermittent, Erratic, or Lumpy.  If you have more than 1 class of time series in your data, this might suggest more than 1 model for your time series predictions.  \n",
    "\n",
    "Rules:\n",
    "<ol>\n",
    "    <li><b>Smooth</b> demand (ADI < 1.32 and CV² < 0.49). Regular in time and in quantity. It is therefore easy to forecast and you won’t have trouble reaching a low forecasting error level. Suggested algorithm: <b>Traditional statistical such as Exponential Smoothing, Prophet, or ARIMA.</b></li>\n",
    "    <li><b>Intermittent demand</b> (ADI >= 1.32 and CV² < 0.49). The demand history shows very little variation in demand quantity but a high variation in the interval between two demands. Though specific forecasting methods tackle intermittent demands, the forecast error margin is considerably higher. Suggested algorithm: <b>Croston smoothing or some newer research approach coupled with adjusted error metric over longer time period.</b></li>\n",
    "    <li><b>Erratic</b> demand (ADI < 1.32 and CV² >= 0.49). The demand has regular occurrences in time with high quantity variations. Your forecast accuracy remains shaky. Suggested algorithm: <b>Deep Learning</b></li>\n",
    "<li><b>Lumpy</b> demand (ADI >= 1.32 and CV² >= 0.49). The demand is characterized by a large variation in quantity and in time. It is actually impossible to produce a reliable forecast, no matter which forecasting tools you use. This particular type of demand pattern is unforecastable. Suggested algorithm: <b>bootstrap</b></li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install squarify\n",
    "import squarify\n",
    "\n",
    "# SCRATCH - choose 4 colors\n",
    "print(\"\\nTHIS IS JUST A TEST TO CHECK COLOR CHOICES...\")\n",
    "colors = colorblind6[0:4]\n",
    "sizes = [40, 30, 5, 25]\n",
    "squarify.plot(sizes, color=colors)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# CPU times: user 2.54 s, sys: 209 ms, total: 2.75 s\n",
    "# Wall time: 2.84 s\n",
    "\n",
    "full_history = local_util.analysis.classify_timeseries_set(\n",
    "    full_history,\n",
    "    time_col=timestamp,\n",
    "    item_id_col=item_id,\n",
    "    other_dimension_cols=[d for d in dims_except_timestamp if d != item_id],\n",
    "    target_col=target_value,\n",
    "    num_dask_partitions=num_partitions,\n",
    "    # In case pandas-based method is too slow and you have sufficient memory available to\n",
    "    # parallelize, set the below `True` to use Dask instead:\n",
    "    use_dask_if_available=False,\n",
    ")\n",
    "full_history.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the *series* (not data points) in each ts_type:\n",
    "type_counts = full_history.groupby(\n",
    "    dims_except_timestamp + ([geolocation] if geolocation else [])\n",
    ")[\"ts_type\"].first().value_counts()\n",
    "\n",
    "# Plot the number of timeseries by type:\n",
    "display(pd.DataFrame({ \"Count\": type_counts, \"Percentage\": 100 * type_counts / type_counts.sum() }))\n",
    "squarify.plot(\n",
    "    label=[f\"{i} = \\n{v / sum(type_counts):.2%}\" for i, v in type_counts.iteritems()],\n",
    "    sizes=type_counts,\n",
    "    color=colors,\n",
    "    text_kwargs={ \"fontsize\": 14 },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore previous full_history:\n",
    "# full_history.drop([\"ADI\", \"CV_square\", \"ts_type\"], inplace=True, axis=1)\n",
    "# full_history.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = min(type_counts.get(\"erratic\", 0), 5)\n",
    "if n_sample > 0:\n",
    "    print(\"SHOWING SAMPLE ERRATIC SERIES\")\n",
    "    erratic_series = full_history.loc[\n",
    "        full_history[\"ts_type\"] == \"erratic\"\n",
    "    ][dims_except_timestamp].drop_duplicates().sample(n_sample)\n",
    "\n",
    "    display(erratic_series)\n",
    "    local_util.plotting.make_plots(\n",
    "        tts,\n",
    "        erratic_series,\n",
    "        target_value_col=target_value,\n",
    "    )\n",
    "else:\n",
    "    print(\"No erratic time series found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = min(type_counts.get(\"smooth\", 0), 5)\n",
    "if n_sample > 0:\n",
    "    print(\"SHOWING SAMPLE SMOOTH SERIES\")\n",
    "    smooth_series = full_history.loc[\n",
    "        full_history[\"ts_type\"] == \"smooth\"\n",
    "    ][dims_except_timestamp].drop_duplicates().sample(n_sample)\n",
    "\n",
    "    display(smooth_series)\n",
    "    local_util.plotting.make_plots(\n",
    "        tts,\n",
    "        smooth_series,\n",
    "        target_value_col=target_value,\n",
    "    )\n",
    "else:\n",
    "    print(\"No smooth time series found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = min(type_counts.get(\"intermittent\", 0), 5)\n",
    "if n_sample > 0:\n",
    "    print(\"SHOWING SAMPLE INTERMITTENT SERIES\")\n",
    "    intermittent_series = full_history.loc[\n",
    "        full_history[\"ts_type\"] == \"intermittent\"\n",
    "    ][dims_except_timestamp].drop_duplicates().sample(n_sample)\n",
    "\n",
    "    display(intermittent_series)\n",
    "    local_util.plotting.make_plots(\n",
    "        tts,\n",
    "        intermittent_series,\n",
    "        target_value_col=target_value,\n",
    "    )\n",
    "else:\n",
    "    print(\"No intermittent time series found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = min(type_counts.get(\"lumpy\", 0), 5)\n",
    "if n_sample > 0:\n",
    "    print(\"SHOWING LUMPY INTERMITTENT SERIES\")\n",
    "    lumpy_series = full_history.loc[\n",
    "        full_history[\"ts_type\"] == \"lumpy\"\n",
    "    ][dims_except_timestamp].drop_duplicates().sample(n_sample)\n",
    "\n",
    "    display(lumpy_series)\n",
    "    local_util.plotting.make_plots(\n",
    "        tts,\n",
    "        lumpy_series,\n",
    "        target_value_col=target_value,\n",
    "    )\n",
    "else:\n",
    "    print(\"No lumpy time series found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like only \"erratic\" time series are worth predicting.  All the rest look either suspiciously fake or too few data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 22. Optional - Assemble and save TTS_smooth, TTS_erratic, TTS_intermittent, TTS_lumpy <a class=\"anchor\" id=\"TTS_classes\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in type_counts.index:\n",
    "    print(f\"\\n#### Subset '{t}' ####\")\n",
    "\n",
    "    subset = full_history.loc[\n",
    "        full_history[\"ts_type\"] == t\n",
    "    ][dims_except_timestamp].drop_duplicates()\n",
    "    num_subset_ts = len(subset)\n",
    "\n",
    "    tts_subset = local_util.dataprep.select_by_df(tts, subset)\n",
    "    num_subset_items = tts_subset[item_id].nunique()\n",
    "    print(f\"Selected {tts_subset.shape} from {tts.shape}\")\n",
    "    print(f\"{num_subset_ts} time-series, {num_subset_items} unique item IDs\")\n",
    "\n",
    "    # Save file locally and upload to S3:\n",
    "    local_file = f\"data/tts_{t}.csv\"\n",
    "    tts_subset.to_csv(local_file, header=False, index=False)\n",
    "    print(f\"Saved {t} subset to {local_file}\")\n",
    "\n",
    "    key = f\"{prefix}/v{DATA_VERSION}/tts_{t}_{EXPERIMENT_NAME}.csv\"\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)\n",
    "    print(f\"Uploaded to s3://{bucket_name}/{key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 23. Assemble and save metadata (if any) <a class=\"anchor\" id=\"IM\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify metadata columns\n",
    "im = df[dims_except_timestamp + ['pickup_borough']].copy()\n",
    "im = im.groupby(dims_except_timestamp).first()\n",
    "im.reset_index(inplace=True)\n",
    "# check nulls\n",
    "display(im.isnull().sum())\n",
    "im.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional metadata created by binning just item target_value is sometimes useful.\n",
    "\n",
    "# aggregate sales by item \n",
    "synthetic = df.copy()\n",
    "synthetic = (synthetic.groupby(item_id).agg({ target_value: [\"max\"] }))\n",
    "\n",
    "synthetic = synthetic.reset_index()\n",
    "synthetic.sample(5)\n",
    "\n",
    "#bin data into 4 categories\n",
    "cat_scales = [\"Cat_{}\".format(i) for i in range(1,5)]\n",
    "synthetic['item_cat_by_max'] = list(pd.cut(synthetic[target_value]['max'].values, 4, labels=cat_scales))\n",
    "\n",
    "synthetic.drop(target_value, axis=1, inplace=True)\n",
    "synthetic.columns = synthetic.columns.get_level_values(0)\n",
    "\n",
    "print(synthetic.shape)\n",
    "print(synthetic.dtypes)\n",
    "print(synthetic.columns)\n",
    "display(synthetic.sample(5))\n",
    "print(synthetic.item_cat_by_max.value_counts(dropna=False))\n",
    "\n",
    "# merge synthetic features\n",
    "im = im.merge(synthetic, how=\"left\", on=[item_id])\n",
    "print(im.shape, synthetic.shape)\n",
    "im.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check metadata so far\n",
    "\n",
    "print(im.shape)\n",
    "if im.shape[0] < 50:\n",
    "    display(im)\n",
    "else:\n",
    "    display(im.head())\n",
    "\n",
    "# check cardinality of metadata columns\n",
    "im.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in sparse or not column\n",
    "im['is_sparse'] = 0\n",
    "\n",
    "im.loc[(im[item_id].isin(list(sparse.index.to_frame()[item_id].unique()))), 'is_sparse'] = 1\n",
    "print(im.is_sparse.value_counts(dropna=False))\n",
    "im.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in top-moving or not column\n",
    "im['top_moving'] = 0\n",
    "\n",
    "im.loc[(im[item_id].isin(list(top_movers.index.to_frame()[item_id].unique()))), 'top_moving'] = 1\n",
    "print(im.top_moving.value_counts(dropna=False))\n",
    "im.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in time series categories column\n",
    "categories_df = full_history.groupby([item_id])[item_id, 'ts_type'].first()\n",
    "categories_df.reset_index(inplace=True, drop=True)\n",
    "# categories_df.head(2)\n",
    "\n",
    "im = im.merge(categories_df, how=\"left\", on=[item_id])\n",
    "print(im.ts_type.value_counts(dropna=False))\n",
    "im.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble metadata just columns you want\n",
    "\n",
    "im = im.iloc[:, 0:3].groupby(item_id).max()\n",
    "im.reset_index(inplace=True)\n",
    "print(im.shape)\n",
    "print(\"checking nulls..\")\n",
    "print(im.isnull().sum())\n",
    "im.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save im to S3\n",
    "local_file = \"data/metadata.csv\"\n",
    "# Save merged file locally\n",
    "im.to_csv(local_file, header=False, index=False)\n",
    "print(f\"Saved metadata to {local_file}\")\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/{EXPERIMENT_NAME}.metadata.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)\n",
    "print(f\"Uploaded to s3://{bucket_name}/{key}\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
